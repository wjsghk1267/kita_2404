{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://startcoding.pythonanywhere.com/basic?page=1&keyword=\n",
      "10\n",
      "10\n",
      "10\n",
      "Page : 1\n",
      "https://startcoding.pythonanywhere.com/basic?page=2&keyword=\n",
      "20\n",
      "20\n",
      "20\n",
      "Page : 2\n",
      "https://startcoding.pythonanywhere.com/basic?page=3&keyword=\n",
      "30\n",
      "30\n",
      "30\n",
      "Page : 3\n",
      "https://startcoding.pythonanywhere.com/basic?page=4&keyword=\n",
      "35\n",
      "35\n",
      "35\n",
      "Page : 4\n"
     ]
    }
   ],
   "source": [
    "# 실습 과제 : Electro 사이트를 크롤링해서 category, name,   link, price 4개 컬럼으로 구성되는 데이터프레임을 출력하세요.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "\n",
    "# url = 'https://startcoding.pythonanywhere.com/basic'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "page = []\n",
    "categories_texts = []\n",
    "name_texts = []\n",
    "link_ = []\n",
    "price_texts = []\n",
    "\n",
    "for p in range(1, 5):\n",
    "    url = f'https://startcoding.pythonanywhere.com/basic?page={p}&keyword='\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # 카테고리\n",
    "        categories = soup.find_all(class_='product-category')\n",
    "        categories_texts.extend([category.get_text() for category in categories])\n",
    "        print(len(categories_texts))\n",
    "        # 상품명\n",
    "        names = soup.find_all(class_='product-name')\n",
    "        name_texts.extend([name.get_text() for name in names])\n",
    "        print(len(name_texts))\n",
    "        # 상품링크\n",
    "        links = soup.find_all('a')\n",
    "        link_.extend([a.get('href') for a in links])\n",
    "        # 가격\n",
    "        prices = soup.find_all(class_='product-price')\n",
    "        for price in prices:\n",
    "            old_prices = price.find('del', class_='product-old-price')\n",
    "            if old_prices:\n",
    "                old_prices.decompose()  # old_price 요소 삭제\n",
    "            \n",
    "            price_texts.append(price.get_text().strip())\n",
    "        \n",
    "        print(len(price_texts))\n",
    "        # 페이지\n",
    "        page.extend([p] * len(categories))\n",
    "\n",
    "    print(f'Page : {p}')\n",
    "\n",
    "data = {\n",
    "    'Page' : page,\n",
    "    'Category': categories_texts,\n",
    "    'Name': name_texts,\n",
    "    # 'Link': link_,\n",
    "    'Price': price_texts\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "df.to_csv('product_info.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
