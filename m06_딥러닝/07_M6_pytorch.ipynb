{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1dEVAgBafR3h4BJK___cnmpeIcAoKJuJk","authorship_tag":"ABX9TyNFbd9CBw8i8oI9ELgCmVXR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## PyTorch vs. Tensorflow/Keras\n","\n","### 1. **PyTorch**\n","\n","#### 1.1. **개요**\n","PyTorch는 Facebook의 AI Research Lab에서 개발한 오픈 소스 딥러닝 프레임워크입니다. Pythonic한 인터페이스와 동적 계산 그래프(dynamic computational graph) 특성 덕분에 학습 곡선이 상대적으로 완만하고, Python 스타일의 코드를 사용할 수 있어서 자연스럽게 익숙해질 수 있습니다. 주로 연구 커뮤니티에서 인기를 얻었고, 최근에는 산업적 사용도 점점 증가하고 있습니다.\n","\n","#### 1.2. **주요 특징**\n","- **동적 계산 그래프 (Dynamic Computation Graph)**\n","  - PyTorch의 가장 큰 특징 중 하나는 동적 계산 그래프입니다. 이를 통해 런타임에 그래프가 생성되므로, 코드 디버깅이나 변경이 훨씬 용이합니다. 이 특성 덕분에 PyTorch는 특히 연구에서 새로운 아이디어를 실험하기에 적합합니다.\n","  \n","- **직관적인 인터페이스**\n","  - Pythonic한 코딩 스타일을 가지고 있으며, 익숙한 Python 문법을 사용합니다. 이로 인해 TensorFlow보다 코드 작성이 상대적으로 간결하고 직관적입니다.\n","  \n","- **TorchScript**\n","  - PyTorch는 TorchScript를 사용하여 Python과 독립적으로 모델을 실행할 수 있게 해줍니다. 이는 모델을 실제 프로덕션 환경에 배포할 때 유용합니다.\n","  \n","- **Autograd**\n","  - PyTorch의 Autograd 기능은 자동 미분을 처리하는데, 이를 통해 백프로퍼게이션 과정에서 파생되는 모든 미분을 자동으로 계산할 수 있습니다.\n","\n","- **커뮤니티**\n","  - 연구 커뮤니티와 긴밀하게 연계되어 있어 많은 논문에서 PyTorch로 구현된 코드가 제공됩니다. 딥러닝 연구에 적합한 프레임워크로 자리 잡고 있습니다.\n","\n","### 2. **TensorFlow / Keras**\n","\n","#### 2.1. **개요**\n","TensorFlow는 Google Brain 팀에서 개발한 오픈 소스 딥러닝 프레임워크로, Keras는 TensorFlow와 통합되어 사용되는 고수준 API입니다. TensorFlow는 생산 환경에서 대규모로 딥러닝 모델을 구축하고 배포하기에 적합하며, Keras는 사용자 친화적인 인터페이스를 제공하여 초보자들도 쉽게 사용할 수 있도록 설계되었습니다.\n","\n","#### 2.2. **주요 특징**\n","- **정적 계산 그래프 (Static Computation Graph)**\n","  - TensorFlow는 초기에는 정적 계산 그래프를 사용했습니다. 즉, 그래프를 먼저 정의하고 나중에 실행하는 방식으로, 한 번 그래프를 정의하면 고정되어 변형이 어렵습니다. 최근에는 TensorFlow 2.0 이후 동적 계산 그래프(Eager Execution)를 도입해 PyTorch처럼 동적 그래프를 지원합니다.\n","  \n","- **확장성과 배포**\n","  - TensorFlow는 대규모 배포 환경에서 매우 강력합니다. TensorFlow Serving, TensorFlow Lite, TensorFlow.js 등 다양한 배포 옵션을 제공하여 모바일, 웹, 임베디드 디바이스까지 모델을 배포할 수 있습니다.\n","  \n","- **Keras (TensorFlow의 고수준 API)**\n","  - Keras는 TensorFlow와 통합된 고수준 API로, 사용자 친화적이고 빠르게 모델을 설계, 구축, 학습할 수 있습니다. 특히 Keras는 초보자들이 쉽게 딥러닝 모델을 시작할 수 있게 해줍니다.\n","  \n","- **TensorBoard**\n","  - TensorFlow는 강력한 시각화 도구인 TensorBoard를 제공합니다. 이를 통해 모델의 학습 과정을 실시간으로 모니터링하고, 그래프 및 파라미터의 변화를 시각적으로 확인할 수 있습니다.\n","\n","- **TensorFlow Hub**\n","  - TensorFlow Hub는 사전 학습된 모델을 재사용할 수 있는 플랫폼으로, 연구자나 개발자들이 이미 학습된 모델을 쉽게 가져와 사용하거나 재학습할 수 있게 도와줍니다.\n","\n","### 3. **PyTorch와 TensorFlow/Keras의 비교**\n","\n","| **비교 항목**           | **PyTorch**                           | **TensorFlow/Keras**                |\n","|------------------------|---------------------------------------|-------------------------------------|\n","| **계산 그래프**         | 동적 계산 그래프 (Dynamic Graph)       | 정적 계산 그래프 (Static Graph) + 동적 지원 (Eager Execution) |\n","| **코딩 스타일**         | Pythonic, 간결하고 직관적             | Keras: 간결, TensorFlow: 다소 복잡함 |\n","| **배포 및 확장성**      | TorchScript로 배포 가능                | TensorFlow Serving, Lite, JS 등 다양한 배포 옵션 |\n","| **자동 미분 (Autograd)**| 매우 직관적                            | 자동 미분 제공하지만, PyTorch만큼 직관적이지 않음 |\n","| **성능**                | 연구 중심, 모델 실험에 적합            | 대규모 배포 환경에서 강력한 성능 제공 |\n","| **커뮤니티**            | 연구 중심, 최신 논문 구현 많이 제공    | 산업계와 연구계 모두 널리 사용됨    |\n","| **시각화 도구**         | TensorBoard 대안은 부족함              | TensorBoard 제공, 강력한 시각화 도구 |\n","| **모바일 및 웹 지원**   | 비교적 미약한 지원                    | TensorFlow Lite, TensorFlow.js 등 폭넓은 지원 |\n","| **프로덕션 환경 지원**  | TorchServe로 배포 가능하지만 한정적     | TensorFlow Extended(TFX)로 대규모 배포 지원 |\n","\n","### 4. **결론**\n","\n","- **PyTorch**는 직관적인 코드 작성과 동적 계산 그래프 덕분에 **연구 및 실험**에 적합합니다. 새로운 아이디어를 빠르게 실험하고 싶거나, 딥러닝 모델을 배우는 데 자연스럽게 접근하고 싶다면 PyTorch가 좋은 선택입니다.\n","- **TensorFlow/Keras**는 **대규모 배포**와 **프로덕션 환경**에서 딥러닝 모델을 사용하는 데 더 적합합니다. 특히 TensorFlow는 다양한 배포 옵션과 확장성을 지원하며, Keras는 간결한 API로 초보자들이 쉽게 접근할 수 있는 장점이 있습니다.\n","\n","둘 다 매우 강력한 딥러닝 프레임워크로, 연구 목적인지, 대규모 배포 목적에 맞는지에 따라 선택할 수 있습니다."],"metadata":{"id":"BQO1W1nMYj3T"}},{"cell_type":"markdown","source":["PyTorch의 **동적 계산 그래프 (Dynamic Computation Graph)**는 딥러닝 모델을 구축하고 학습할 때 PyTorch가 제공하는 매우 중요한 기능 중 하나입니다. 이는 \"즉시 실행 (eager execution)\"이라고도 불리며, TensorFlow와 같은 프레임워크의 정적 계산 그래프(static computation graph)와 대비됩니다.\n","\n","### 동적 계산 그래프의 핵심 개념\n","동적 계산 그래프는 학습 중에 그래프를 즉시 정의하고 실행하는 방식입니다. 즉, 계산을 수행할 때마다 새로운 그래프를 만들고, 이를 바로 실행하는 특징을 갖고 있습니다. 이를 통해 계산 과정 중 변수나 모델 구조의 변경이 매우 유연하게 이루어질 수 있습니다.\n","\n","#### 1. **즉시 실행 (Eager Execution)**\n","PyTorch는 코드가 실행될 때마다 계산 그래프를 동적으로 생성하고, 이를 즉시 실행합니다. 즉, 딥러닝 모델에서 데이터를 전방 패스로 전달할 때, 해당 연산이 바로 수행되고 그 결과가 즉시 반환됩니다. 이런 즉시 실행 방식은 디버깅을 매우 용이하게 만들어 줍니다.\n","\n","예를 들어, 다음과 같은 코드를 생각해 봅시다:\n","\n","```python\n","import torch\n","\n","x = torch.tensor(2.0, requires_grad=True)\n","y = torch.tensor(3.0, requires_grad=True)\n","\n","z = x * y + y\n","```\n","\n","여기서 `z`를 계산할 때 `x * y + y`가 바로 실행되고, 이 과정에서 계산 그래프가 동적으로 생성됩니다. 이후 `z.backward()`를 호출하면, PyTorch는 자동으로 그래프를 통해 미분을 수행하여 각 변수의 gradient를 계산합니다.\n","\n","#### 2. **유연성 (Flexibility)**\n","동적 계산 그래프는 모델 구조를 즉석에서 정의하고, 계산 과정 중에도 그래프의 변경이 가능합니다. 이는 복잡한 구조를 가진 모델을 구현할 때 매우 유용합니다. 예를 들어, if문이나 for문을 통해 네트워크의 구조를 유연하게 변경할 수 있습니다.\n","\n","```python\n","for i in range(5):\n","    if i % 2 == 0:\n","        z = z * x\n","    else:\n","        z = z + y\n","```\n","\n","이 코드는 실행할 때마다 그래프의 구조가 바뀔 수 있는데, PyTorch의 동적 계산 그래프는 이러한 경우에도 문제없이 작동합니다. 이는 정적 계산 그래프에서 불가능하거나 복잡한 코드로 구현해야 하는 것을 동적 계산 그래프에서는 자연스럽게 구현할 수 있게 해줍니다.\n","\n","#### 3. **Autograd: 자동 미분 기능**\n","PyTorch에서 동적 계산 그래프는 **Autograd** 기능과 밀접하게 연결되어 있습니다. Autograd는 계산 그래프를 추적하여, 미분이 필요한 연산에 대한 그래프를 자동으로 만들어 줍니다. 연산이 수행되면 PyTorch는 이를 기록하고, `backward()` 함수를 호출할 때 그래프를 따라 역전파를 통해 기울기(gradient)를 계산합니다.\n","\n","```python\n","z.backward()  # z의 연산을 바탕으로 역전파 수행\n","print(x.grad)  # x의 gradient 값 출력\n","```\n","\n","Autograd는 동적으로 그래프를 구성하기 때문에, 계산 그래프는 전방 패스(forward pass) 중에 실시간으로 생성되고, 역전파(backward pass)가 끝나면 필요에 따라 버려집니다. 이는 메모리 효율성을 높이고, 메모리 관리를 쉽게 해줍니다.\n","\n","### 동적 계산 그래프의 장점\n","- **디버깅이 용이함**: 계산이 즉시 실행되므로, 중간 결과를 쉽게 확인할 수 있어 디버깅 과정이 매우 직관적입니다. 오류가 발생하면 즉시 알 수 있습니다.\n","- **유연한 모델 설계**: 반복문, 조건문 등을 사용해 네트워크 구조를 동적으로 변경할 수 있으며, 이는 매우 복잡한 모델을 쉽게 구현하는 데 도움을 줍니다.\n","- **학습 시 효율적**: 필요할 때만 그래프가 생성되고 역전파에 사용되며, 이후 메모리에서 제거되므로 메모리 사용이 효율적입니다.\n","\n","### 정적 계산 그래프와의 차이점\n","- **정적 계산 그래프 (TensorFlow, Theano)**는 모델을 먼저 정의한 후, 그래프 전체를 컴파일하여 실행합니다. 이런 방식은 실행 전에 최적화된 모델을 만들 수 있지만, 그래프를 수정하려면 다시 컴파일해야 하므로 덜 유연합니다.\n","- 반면, **동적 계산 그래프 (PyTorch)**는 실행 시점에서 그래프를 생성하므로 더 유연하지만, 실행 속도는 정적 그래프에 비해 다소 느릴 수 있습니다. 하지만 최근에는 PyTorch에서도 정적 그래프처럼 최적화를 할 수 있는 방법이 연구되고 있습니다(예: TorchScript).\n","\n","### PyTorch의 동적 계산 그래프 작동 방식\n","PyTorch의 동적 계산 그래프는 연산을 할 때마다 다음과 같은 과정을 거칩니다:\n","\n","1. **Tensor와 연산 정의**: 계산 그래프는 Tensor와 연산으로 구성됩니다. Tensor는 `requires_grad=True` 옵션을 통해 그라디언트를 추적할지 여부를 설정할 수 있습니다.\n","2. **전방 패스 실행**: 전방 패스가 실행되면서 연산 결과가 생성되고, 그와 동시에 계산 그래프가 생성됩니다.\n","3. **역전파 실행**: `backward()` 호출 시, 계산 그래프를 타고 내려가면서 그라디언트가 자동으로 계산됩니다.\n","\n","결론적으로, PyTorch의 동적 계산 그래프는 코드가 실행될 때마다 실시간으로 그래프를 구성하여 유연한 모델 설계와 디버깅을 가능하게 합니다. 이 특성 덕분에 PyTorch는 연구나 개발 환경에서 널리 사용되며, 특히 복잡하고 유연한 모델 설계에 적합한 딥러닝 프레임워크로 자리 잡았습니다."],"metadata":{"id":"D1dT3MDSboh2"}},{"cell_type":"code","source":["# 여기서 z를 계산할때 x*y+y가 바로 실행되고, 이 과정에서 계산 그래프가 동적으로 생성\n","# 이후 z, backward()를 호출하면, Pytorch가 자동으로 그래프를 토해 미분을 수행하며 각 변수의 기울기를 계산\n","import torch\n","\n","x = torch.tensor(2.0, requires_grad=True)\n","y = torch.tensor(3.0, requires_grad=True)\n","\n","z = x * y + y"],"metadata":{"id":"jdZrO9tnc53O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 동적 계산 그래프는 모델 구조를 즉석에서 정의하고, 계산 과정 중에도 그래프의 변경이 가능.\n","# if 문이나 for문을 통해 네트워크 구조를 유연하게 변경할 수 있습니다.\n","for i in range(5):\n","    if i % 2 == 0:\n","        z = z * x\n","    else:\n","        z = z + y"],"metadata":{"id":"QJfZDNkSdHyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# autograd는 계산 그래프를 추적하여, 미분이 필요한 연산에 대한 그래프를 자동으로 만들어 줌.\n","# 연산이 수행되면 pytorch는 이를 기록하고, backward() 함수를 호출할 때, 그래프를 따라 역전파를 통해 기울기를 계산\n","z.backward() # z 연산을 바탕으로 역전파 수행\n","print(x.grad) # x의 기울기값 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VirjtXz5dSU5","executionInfo":{"status":"ok","timestamp":1724722318238,"user_tz":-540,"elapsed":662,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"7b6e11e0-7ceb-4909-adc1-7613033e1615"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(147.)\n"]}]},{"cell_type":"markdown","source":["계산 그래프의 사례를 간단한 수식과 함께 살펴볼 수 있습니다. 예를 들어, 다음과 같은 수식을 고려해 보겠습니다:\n","\n","\\[\n","z = (x + y) * w\n","\\]\n","\n","여기서 \\(x\\), \\(y\\), \\(w\\)는 입력 변수이고, \\(z\\)는 결과입니다. 이 수식을 계산 그래프로 나타내면, 각 연산이 그래프의 노드로 표현되고, 변수와 연산 사이의 의존 관계가 엣지로 연결됩니다. 이제 이 수식을 계산 그래프로 표현한 사례를 보여드리겠습니다:\n","\n","### 수식: \\( z = (x + y) * w \\)\n","\n","1. **입력 노드**: \\(x\\), \\(y\\), \\(w\\)는 그래프의 입력 변수입니다.\n","2. **덧셈 연산 노드**: 먼저, \\(x\\)와 \\(y\\)를 더하는 연산이 일어납니다.\n","3. **곱셈 연산 노드**: 그다음, 덧셈 결과와 \\(w\\)를 곱하는 연산이 일어납니다.\n","4. **출력 노드**: 최종 결과인 \\(z\\)가 나옵니다.\n","\n","### 계산 그래프의 시각적 표현\n","다음과 같이 계산 과정을 그래프로 나타낼 수 있습니다:\n","\n","```\n","   x -----\n","          |\n","          + ----> (x + y) -----> * ----> z\n","          |                      |\n","   y -----                       w\n","```\n","\n","여기서 각 단계는 다음과 같습니다:\n","- **덧셈 연산**: \\(x + y\\)는 하나의 노드로 표현됩니다.\n","- **곱셈 연산**: 그다음 덧셈 결과와 \\(w\\)의 곱이 또 다른 노드로 표현됩니다.\n","- **출력**: 최종 결과인 \\(z\\)가 마지막에 나옵니다.\n","\n","### 역전파 과정\n","이 계산 그래프는 **역전파** 과정에서 중요한 역할을 합니다. 예를 들어, 손실 함수에서 \\(z\\)에 대한 기울기를 계산할 때, 연쇄 법칙에 따라 각 연산에 대한 기울기를 차례대로 계산하여 전달합니다.\n","\n","1. **\\(z\\)에 대한 기울기 계산**: 먼저, \\(z\\)에 대한 손실 함수의 기울기를 구합니다.\n","2. **곱셈 연산의 기울기**: \\(z\\)의 기울기를 \\(w\\)와 \\((x + y)\\) 각각에 대해 계산하여 전달합니다.\n","3. **덧셈 연산의 기울기**: \\((x + y)\\)의 기울기를 \\(x\\)와 \\(y\\)에 대해 계산하여 전달합니다.\n","\n","이 과정에서 PyTorch와 같은 프레임워크는 자동으로 역전파를 수행해 각 파라미터에 대한 기울기를 구할 수 있습니다.\n","\n","### 더 복잡한 사례\n","실제 딥러닝 모델에서는 이보다 훨씬 복잡한 계산 그래프가 형성됩니다. 예를 들어, **합성곱 신경망(CNN)**이나 **순환 신경망(RNN)**은 여러 층(layer)과 비선형 활성화 함수, 매트릭스 연산 등이 포함되어 있으며, 이러한 모든 연산이 하나의 거대한 계산 그래프로 연결됩니다. 각 층의 가중치와 입력 간의 연산들이 일련의 계산 그래프를 형성하고, 이 그래프를 통해 역전파가 이루어집니다.\n","\n","---\n","\n","### 요약\n","계산 그래프는 모델의 연산을 시각적으로 표현한 것이며, 이 그래프를 통해 딥러닝 모델의 학습 과정에서 자동 미분과 역전파가 가능합니다."],"metadata":{"id":"VMbp_Xlkd7x6"}},{"cell_type":"markdown","source":["## 파이토치의 구성요소\n","\n","- `torch`: 메인 네임스페이스, 텐서 등의 다양한 수학 함수가 포함\n","- `torch.autograd`: 자동 미분 기능을 제공하는 라이브러리\n","- `torch.nn`: 신경망 구축을 위한 데이터 구조나 레이어 등의 라이브러리\n","- `torch.multiprocessing`: 병럴처리 기능을 제공하는 라이브러리\n","- `torch.optim`: SGD(Stochastic Gradient Descent)를 중심으로 한 파라미터 최적화 알고리즘 제공\n","- `torch.utils`: 데이터 조작 등 유틸리티 기능 제공\n","- `torch.onnx`: ONNX(Open Neural Network Exchange), 서로 다른 프레임워크 간의 모델을 공유할 때 사용"],"metadata":{"id":"5RdLkqhqgyXZ"}},{"cell_type":"markdown","source":["3D Tensor\n","\n","* 큐브(cube)와 같은 모양으로 세개의 축이 존재\n","* 데이터가 연속된 시퀀스 데이터나 시간 축이 포함된 시계열 데이터에 해당\n","* 주식 가격 데이터셋, 시간에 따른 질병 발병 데이터 등이 존재\n","* 주로 샘플(samples), 타임스텝(timesteps), 특성(features)을 가진 구조로 사용\n","\n","4D Tensor\n","\n","* 4개의 축\n","* 컬러 이미지 데이터가 대표적인 사례 (흑백 이미지 데이터는 3D Tensor로 가능)\n","* 주로 샘플(samples), 높이(height), 너비(width), 컬러 채널(channel)을 가진 구조로 사용\n","\n","5D Tensor\n","\n","* 5개의 축\n","* 비디오 데이터가 대표적인 사례\n","* 주로 샘플(samples), 프레임(frames), 높이(height), 너비(width), 컬러 채널(channel)을 가진 구조로 사용"],"metadata":{"id":"9jPDJpc_hsYA"}},{"cell_type":"code","source":["# 텐서에 대한 수학연산, 삼각함수, 비트연산, 비교연산, 집계등 제공\n","a = torch.randn(1,2) * 2 -1 # 표준 정규분포(평균=0, 표준편차 =1)에서 가져온 값의 각요소에 2를 곱하고 -1한다.\n","print(a)\n","print(torch.abs(a))\n","print(torch.ceil(a)) # 텐서 a의 각 요소에 천장 함수 적용\n","print(torch.floor(a)) # 텐서 a의 각 요소에 바닥함수 적용\n","print(torch.clamp(a, -0.5, 0.5)) # -0.5 이하의 값은 -0.5로 설정, 0.5 이상은 0.5로 설정"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3VwQIyUqmkSg","executionInfo":{"status":"ok","timestamp":1724724795736,"user_tz":-540,"elapsed":489,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"c05a5960-bae6-4f5d-bd0a-0b2a76ce4967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.9615, -3.1051]])\n","tensor([[0.9615, 3.1051]])\n","tensor([[ 1., -3.]])\n","tensor([[ 0., -4.]])\n","tensor([[ 0.5000, -0.5000]])\n"]}]},{"cell_type":"markdown","source":["- in-place방식으로 텐서의 값을 변경하는 연산뒤에는 _ 가 붙음\n","- x,copy_(y), x.t_()"],"metadata":{"id":"Bp4sNLHMnfHc"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.tensor([1.0, -1.0, 0.5])\n","\n","# 일반 연산 (새로운 텐서 생성)\n","y = x.abs() # 절대값을 구하지만 x는 그대로\n","print(x) # tensor([1.0, -1.0, 0.5])\n","print(y) # tesnor([1.0, 1.0, 0.5])\n","\n","# in-place 연산\n","x.abs_() # 기존 텐서 x의 값을 직접 변경\n","print(x) # tensor([1.0, 1.0, 0.5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e882Mr8pnocb","executionInfo":{"status":"ok","timestamp":1724725060506,"user_tz":-540,"elapsed":568,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"3275b343-4a6c-4e87-a56e-43e8f2bc0b45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.0000, -1.0000,  0.5000])\n","tensor([1.0000, 1.0000, 0.5000])\n","tensor([1.0000, 1.0000, 0.5000])\n"]}]},{"cell_type":"code","source":["x = torch.rand(2,2) # 0,1 사이의 값을 가지는 균등분포로부터 무작위로 생성된 행렬을 반환\n","print(x)\n","y=torch.rand(2,2)\n","print(y)\n","\n","print()\n","y.add_(x) # _가 있는 add_는 매서드가 y자체를 변경한다는 의여서 y값이 변경\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBso22ZAomhY","executionInfo":{"status":"ok","timestamp":1724725294461,"user_tz":-540,"elapsed":504,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"688b3b32-a174-4072-f5af-26944cfb6bb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4680, 0.2257],\n","        [0.9361, 0.8527]])\n","tensor([[0.2724, 0.7544],\n","        [0.4864, 0.5971]])\n","\n","tensor([[0.7404, 0.9801],\n","        [1.4225, 1.4498]])\n"]}]},{"cell_type":"code","source":["# 인덱싱 Numpy처럼 인덱싱 형태로 사용가능\n","x = torch.Tensor([[1,2],[3,4]])\n","print(x)\n","\n","print(x[0,0])\n","print(x[1,0])\n","print(x[0,1])\n","print(x[0,:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjC1_oXdpajt","executionInfo":{"status":"ok","timestamp":1724725486561,"user_tz":-540,"elapsed":846,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"3f01c358-17bf-43d9-b9b6-7345b6a8ad0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.]])\n","tensor(1.)\n","tensor(3.)\n","tensor(2.)\n","tensor([1., 2.])\n"]}]},{"cell_type":"markdown","source":["º 랜덤한 값을 가지는 텐서 생성\n","\n","1. torch.rand() : 0과 1 사이의 숫자를 균등하게 생성\n","\n","2. torch.rand_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n","\n","3. torch.randn() : 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용해 생성\n","\n","4. torch.randn_like() :  사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n","\n","5. torch.randint() : 주어진 범위 내의 정수를 균등하게 생성\n","\n","6. torch.randint_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n","\n","7. torch.randperm() : 주어진 범위 내의 정수를 랜덤하게 생성"],"metadata":{"id":"bXs4UsVPq-Es"}},{"cell_type":"code","source":["tensor_rand = torch.rand(3,4)\n","existing_tensor = torch.tensor([[1,2],[3,4]])\n","tensor_rand_like = torch.rand_like(existing_tensor, dtype=torch.float)\n","tensor_randn = torch.randn(3,3)\n","tensor_randn_like = torch.randn_like(existing_tensor, dtype=torch.float)\n","tensor_randint = torch.randint(0, 10, (3,3))\n","tensor_randint_like = torch.randint_like(existing_tensor, low=0, high=10, dtype=torch.int)\n","tensor_randperm = torch.randperm(10) # 0부터 n-1까지의 정수를 무작위로 섞은 순열을 반환, 중복없는 무작위 정수 시퀀스\n","\n","tensor_rand, tensor_rand_like, tensor_randn, tensor_randn_like, tensor_randint, tensor_randint_like, tensor_randperm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WndUJ57qq-a1","executionInfo":{"status":"ok","timestamp":1724726013391,"user_tz":-540,"elapsed":665,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"25591877-fe2a-433f-aadf-306ee0a81b5e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.2855, 0.6711, 0.8761, 0.2436],\n","         [0.8381, 0.2733, 0.3719, 0.6312],\n","         [0.3925, 0.2995, 0.6521, 0.8739]]),\n"," tensor([[0.3768, 0.1739],\n","         [0.9485, 0.5150]]),\n"," tensor([[ 2.1589,  1.0190,  0.7446],\n","         [ 2.0375,  0.7151,  1.1793],\n","         [-1.2012,  0.4010,  1.7957]]),\n"," tensor([[ 0.6337,  0.6397],\n","         [-1.2512,  0.3612]]),\n"," tensor([[3, 4, 4],\n","         [7, 4, 1],\n","         [7, 4, 3]]),\n"," tensor([[7, 7],\n","         [4, 8]], dtype=torch.int32),\n"," tensor([5, 8, 3, 2, 9, 1, 7, 0, 6, 4]))"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["º 특정한 값을 가지는 텐서 생성\n","\n","1. torch.arange() : 주어진 범위 내의 정수를 순서대로 생성\n","\n","2. torch.ones() : 주어진 사이즈의 1로 이루어진 텐서 생성\n","\n","3. torch.zeros() : 주어진 사이즈의 0으로 이루어진 텐서 생성\n","\n","4. torch.ones_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n","\n","5. torch.zeros_like() : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n","\n","6. torch.linspace() : 시작점과 끝점을 주어진 갯수만큼 균등하게 나눈 간격점을 행벡터로 출력\n","\n","7. torch.logspace() : 시작점과 끝점을 주어진 갯수만큼 로그간격으로 나눈 간격점을 행벡터로 출력"],"metadata":{"id":"gs7JVS_NtCc5"}},{"cell_type":"code","source":["tensor_arange = torch.arange(0,10)\n","tensor_ones = torch.ones(3,3)\n","tensor_zeros = torch.zeros(3,3)\n","tensor_ones_like = torch.ones_like(existing_tensor)\n","tensor_zeros_like = torch.zeros_like(existing_tensor)\n","tensor_linspace = torch.linspace(0, 10, steps=5)\n","tensor_logspace = torch.logspace(start=-1, end =1, steps=5)\n","\n","tensor_arange, tensor_ones, tensor_zeros, tensor_ones_like, tensor_zeros_like, tensor_linspace, tensor_logspace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vjwXIRstDyB","executionInfo":{"status":"ok","timestamp":1724726501355,"user_tz":-540,"elapsed":507,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"2f6f3d7b-45b4-4d3a-a798-b8892c48b96f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n"," tensor([[1., 1., 1.],\n","         [1., 1., 1.],\n","         [1., 1., 1.]]),\n"," tensor([[0., 0., 0.],\n","         [0., 0., 0.],\n","         [0., 0., 0.]]),\n"," tensor([[1, 1],\n","         [1, 1]]),\n"," tensor([[0, 0],\n","         [0, 0]]),\n"," tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000]),\n"," tensor([ 0.1000,  0.3162,  1.0000,  3.1623, 10.0000]))"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["view : 텐서의 size나 shape을 변경\n","- 깁노적으로 변경 전/후 텐서 안의 원소개수가 유지되야됨.\n","- -1로 설정되면 계산을 통해 해당 크기값을 유추"],"metadata":{"id":"wIF6iQTOt6fH"}},{"cell_type":"code","source":["x = torch.randn(4,5)\n","print(x,'\\n')\n","\n","y= x.view(20)\n","print(y,'\\n')\n","\n","z = x.view(5,-1)\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ipxYrfRuFum","executionInfo":{"status":"ok","timestamp":1724726691431,"user_tz":-540,"elapsed":868,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"9f59a675-6990-48b1-b820-d6dd30dcb409"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.7170, -0.2895,  0.0298,  0.5155, -0.1791],\n","        [ 2.2376,  0.6939,  0.7278,  0.0705, -0.5622],\n","        [ 0.1387, -0.7555, -0.8363, -0.8529, -1.3926],\n","        [ 0.1852,  0.4623, -0.8867, -0.2223, -1.6280]]) \n","\n","tensor([-0.7170, -0.2895,  0.0298,  0.5155, -0.1791,  2.2376,  0.6939,  0.7278,\n","         0.0705, -0.5622,  0.1387, -0.7555, -0.8363, -0.8529, -1.3926,  0.1852,\n","         0.4623, -0.8867, -0.2223, -1.6280]) \n","\n","tensor([[-0.7170, -0.2895,  0.0298,  0.5155],\n","        [-0.1791,  2.2376,  0.6939,  0.7278],\n","        [ 0.0705, -0.5622,  0.1387, -0.7555],\n","        [-0.8363, -0.8529, -1.3926,  0.1852],\n","        [ 0.4623, -0.8867, -0.2223, -1.6280]])\n"]}]},{"cell_type":"code","source":["# item 텐서에 값이 단 하나라도 존재하면 숫자값을 얻을수있음.\n","tensor = torch.randn(1)\n","print(x)\n","print(x.item()) # item()함수는 텐서의 값을 파이썬으 ㅣ숫자로 반환하여 텐서에서 값만 추출\n","print(x.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"AlmZB9A6u2Z-","executionInfo":{"status":"error","timestamp":1724727105528,"user_tz":-540,"elapsed":518,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"4fed926b-ab07-40dc-c990-85871281a93e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.7170, -0.2895,  0.0298,  0.5155, -0.1791],\n","        [ 2.2376,  0.6939,  0.7278,  0.0705, -0.5622],\n","        [ 0.1387, -0.7555, -0.8363, -0.8529, -1.3926],\n","        [ 0.1852,  0.4623, -0.8867, -0.2223, -1.6280]])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"a Tensor with 20 elements cannot be converted to Scalar","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-afcf953a4820>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# item()함수는 텐서의 값을 파이썬으 ㅣ숫자로 반환하여 텐서에서 값만 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 20 elements cannot be converted to Scalar"]}]},{"cell_type":"markdown","source":["squeeze() 함수는 크기가 1인 입력 텐서의 모든 차원을 제거하는 데 사용\n","- dim이 제공되지 않으면 squeeze()는 텐서에서 크기 1의 모든 차원을 제거\n","- dim이 제공되면 크기가 1인 경우에만 지정된 차원만 제거. 즉 (1, 2, 1, 3)인 텐서가 있고 dim이 2이면 결과의 크기는 (1, 2, 3)"],"metadata":{"id":"P18p8sOXvsDP"}},{"cell_type":"code","source":["# squeeze 차원 축소\n","tensor = torch.rand(1,3,3)\n","print(tensor)\n","print(tensor.shape)\n","t = tensor.squeeze()\n","print(t)\n","print(t.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99OKOzLCvqow","executionInfo":{"status":"ok","timestamp":1724727165288,"user_tz":-540,"elapsed":3,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"9d084f69-6cde-417f-d74a-5d6e8a095f64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.3480, 0.7076, 0.0605],\n","         [0.5060, 0.9693, 0.5366],\n","         [0.7439, 0.0425, 0.2471]]])\n","torch.Size([1, 3, 3])\n","tensor([[0.3480, 0.7076, 0.0605],\n","        [0.5060, 0.9693, 0.5366],\n","        [0.7439, 0.0425, 0.2471]])\n","torch.Size([3, 3])\n"]}]},{"cell_type":"markdown","source":["tensor.unsqueeze(dim=2)\n","\n","- unsqueeze 함수는 텐서에 새로운 차원을 추가. 여기서 dim=2는 새로운 차원이 추가될 위치를 나타내며 기존 텐서의 모든 차원의 인덱스를 증가시키고, 새로운 차원은 dim=2에 위치.\n","\n","- 원래 텐서의 모양(shape)이 (3, 4)이었다면, unsqueeze(dim=2)를 적용한 후의 모양은 (3, 4, 1)이 됩니다. 따라서, 이 코드에서는 기존 텐서에 1차원을 추가"],"metadata":{"id":"2nxVxCii2Okf"}},{"cell_type":"code","source":["t = torch.rand(3,3)\n","print(t)\n","print(t.shape)\n","tensor = t.unsqueeze(dim=0)\n","print(tensor)\n","print(tensor.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umZEaWR12O_R","executionInfo":{"status":"ok","timestamp":1724728831367,"user_tz":-540,"elapsed":665,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"6adf8777-8bc6-4146-bdfd-7a1ac2673344"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1887, 0.1488, 0.3025],\n","        [0.7558, 0.1278, 0.6569],\n","        [0.0561, 0.9578, 0.6951]])\n","torch.Size([3, 3])\n","tensor([[[0.1887, 0.1488, 0.3025],\n","         [0.7558, 0.1278, 0.6569],\n","         [0.0561, 0.9578, 0.6951]]])\n","torch.Size([1, 3, 3])\n"]}]},{"cell_type":"code","source":["tensor.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRwreY_b2Xm3","executionInfo":{"status":"ok","timestamp":1724728838977,"user_tz":-540,"elapsed":2,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"92b6ff5f-8ea0-44f8-b8be-ff34c9a23381"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 3])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["tensor = tensor.unsqueeze(dim=2)\n","print(tensor)\n","print(tensor.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlqu6xIp23uU","executionInfo":{"status":"ok","timestamp":1724728987151,"user_tz":-540,"elapsed":547,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"bbb30e5a-9e74-4e5a-ba73-846067fe19f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[0.1887, 0.1488, 0.3025]],\n","\n","         [[0.7558, 0.1278, 0.6569]],\n","\n","         [[0.0561, 0.9578, 0.6951]]]])\n","torch.Size([1, 3, 1, 3])\n"]}]},{"cell_type":"code","source":["#Q. 아래 텐서를 torch.shize([3,5])로 변경\n","# 초기 텐서\n","tensor_for_squeeze = torch.randn(1,3,1,5)\n","print(\"차원 축소 전 텐서: \\n\", tensor_for_squeeze)\n","print(\"차원 축소 전 형태:\", tensor_for_squeeze.shape )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JB3_0bsc29Ni","executionInfo":{"status":"ok","timestamp":1724729076529,"user_tz":-540,"elapsed":635,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"b1b929c1-00c2-4b7b-8a32-4f07815d5dda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["차원 축소 전 텐서: \n"," tensor([[[[ 1.4670,  0.6131,  0.8921, -0.4545,  0.7876]],\n","\n","         [[-1.5060,  0.5035, -0.4459, -0.7317, -0.4076]],\n","\n","         [[ 0.1544,  0.9409,  0.5178,  0.3828, -0.5221]]]])\n","차원 축소 전 형태: torch.Size([1, 3, 1, 5])\n"]}]},{"cell_type":"code","source":["# torch.squeeze() 모든 크기 1인 차원을 자동으로 제거하지만, 특정 차원만 제거하고 싶다면 차원을 지정할 수도 있음.\n","squeezed_tensor = torch.squeeze(tensor_for_squeeze)\n","print(squeezed_tensor.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j4oSI8D3klE","executionInfo":{"status":"ok","timestamp":1724729194230,"user_tz":-540,"elapsed":565,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"d9828df5-d008-4d30-828a-0a41ad30f6d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 5])\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# 아래 텐서를 torch.size([1,3,5])로 변경.\n","tensor_for_unsqueeze = torch.randn(3,5)\n","print(\"차원 확장 전 텐서: \\n\", tensor_for_unsqueeze)\n","print(\"차원 확장 전 형태:\", tensor_for_unsqueeze.shape)\n","\n","tensor_unsqueezed = tensor_for_unsqueeze.unsqueeze(0)\n","print(\"\\n차원 확장 후 텐서: \\n\", tensor_unsqueezed)\n","print(\"차원 확장 후 형태:\", tensor_unsqueezed.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5Zrv7gz3yC6","executionInfo":{"status":"ok","timestamp":1724801141358,"user_tz":-540,"elapsed":5321,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"3644d74c-def2-4785-e8d6-82da674432c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["차원 확장 전 텐서: \n"," tensor([[-1.1248,  0.6662, -0.4363,  2.4000, -1.3106],\n","        [ 0.9673, -0.4135,  0.1832, -0.6198, -1.1250],\n","        [ 1.9686, -1.1538,  2.1803, -1.3124,  0.8922]])\n","차원 확장 전 형태: torch.Size([3, 5])\n","\n","차원 확장 후 텐서: \n"," tensor([[[-1.1248,  0.6662, -0.4363,  2.4000, -1.3106],\n","         [ 0.9673, -0.4135,  0.1832, -0.6198, -1.1250],\n","         [ 1.9686, -1.1538,  2.1803, -1.3124,  0.8922]]])\n","차원 확장 후 형태: torch.Size([1, 3, 5])\n"]}]},{"cell_type":"markdown","source":["Task1_0827. 2행 3열 텐서의 크기를 3행 2열로 변경하세요.\n","- view 사용\n","- reshape 사용\n","- transpose 사용"],"metadata":{"id":"U-fookpb4DWw"}},{"cell_type":"code","source":["import torch\n","\n","# 2x3 텐서 생성\n","tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print(\"차원 변경 전 텐서:\\n\",tensor)\n","\n","# 3x2 텐서로 크기 변경\n","tensor_view = tensor.view(3, 2)\n","print(\"view:\\n\",tensor_view)\n","\n","tensor_reshape = tensor.reshape(3, 2)\n","print(\"reshape:\\n\",tensor_reshape)\n","\n","tensor_transpose = tensor.transpose(0, 1)\n","print(\"transpose:\\n\",tensor_transpose)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpWGvNu-KPiJ","executionInfo":{"status":"ok","timestamp":1724801263475,"user_tz":-540,"elapsed":394,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"0cc8c3a8-0958-458b-fdc0-56493ac0767f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["차원 변경 전 텐서:\n"," tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","view:\n"," tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","reshape:\n"," tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","transpose:\n"," tensor([[1, 4],\n","        [2, 5],\n","        [3, 6]])\n"]}]},{"cell_type":"markdown","source":["stack은 새로운 차원을 추가하여 텐서들을 결합하는 반면, cat은 기존 차원을 따라 텐서들을 연결\n","\n","- stack : torch.stack()은 새로운 차원을 추가한 후 텐서를 결합합니다. 즉, 텐서들의 차원 수가 1 증가하며, 지정된 차원에 새로운 차원을 추가한 뒤 해당 차원을 따라 텐서들을 쌓습니다.\n","- cat: torch.cat()은 기존의 차원을 따라 텐서들을 연결합니다. 즉, 텐서의 차원 수는 그대로 유지되며, 지정된 차원(axis)에서 텐서들이 연결됩니다."],"metadata":{"id":"sp_aklprXS4b"}},{"cell_type":"code","source":["import torch\n","\n","# 자동으로 데이터 타입 추론\n","t = torch.tensor([1,4])\n","print(t,t.dtype)\n","f=torch.FloatTensor([1,4])\n","print(f,f.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"341NkU8LXTye","executionInfo":{"status":"ok","timestamp":1724805005502,"user_tz":-540,"elapsed":458,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"eef7d3ed-41df-4560-c7e7-4964d82b05cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 4]) torch.int64\n","tensor([1., 4.]) torch.float32\n"]}]},{"cell_type":"code","source":["x = torch.FloatTensor([1,4])\n","print(x)\n","y = torch.FloatTensor([2,5])\n","print(y)\n","z = torch.FloatTensor([3,6])\n","print(z)\n","print(torch.stack([x,y,z]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rR_8aGliXWQC","executionInfo":{"status":"ok","timestamp":1724805008913,"user_tz":-540,"elapsed":409,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"860e1264-98e7-48fc-dddb-259decaa0ef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 4.])\n","tensor([2., 5.])\n","tensor([3., 6.])\n","tensor([[1., 4.],\n","        [2., 5.],\n","        [3., 6.]])\n"]}]},{"cell_type":"markdown","source":["chunk: torch.chunk()는 텐서를 주어진 개수의 균등한 크기로 나눕니다. 나누어진 텐서들의 크기는 가능한 한 동일하게 유지되며, 전체 크기를 나누어 떨어지지 않으면 마지막 텐서는 더 작은 크기가 될 수 있습니다.(몇 개로 나눌 것인가?)\n","\n","split: torch.split()은 텐서를 주어진 크기를 기준으로 나눕니다. 분할되는 각 텐서의 크기를 직접 지정할 수 있습니다. 크기가 균등하지 않아도 되며, 마지막 텐서의 크기는 자동으로 조정됩니다. (텐서의 크기는 몇인가?)"],"metadata":{"id":"iL9ntJuxaAaH"}},{"cell_type":"code","source":["tensor = torch.rand(3,6)\n","print(tensor)\n","\n","t1,t2 = torch.chunk(tensor,2,dim=1)\n","print(t1, t1.shape)\n","print(t2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M9onwFiIZ3DX","executionInfo":{"status":"ok","timestamp":1724805279720,"user_tz":-540,"elapsed":405,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"6e20cadc-c760-449c-ccf1-e9b7169e5d43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2675, 0.2477, 0.7480, 0.0560, 0.3713, 0.4700],\n","        [0.8075, 0.3485, 0.8822, 0.4482, 0.0164, 0.4389],\n","        [0.4997, 0.1598, 0.7352, 0.5715, 0.5960, 0.0944]])\n","tensor([[0.2675, 0.2477, 0.7480],\n","        [0.8075, 0.3485, 0.8822],\n","        [0.4997, 0.1598, 0.7352]]) torch.Size([3, 3])\n","tensor([[0.0560, 0.3713, 0.4700],\n","        [0.4482, 0.0164, 0.4389],\n","        [0.5715, 0.5960, 0.0944]])\n"]}]},{"cell_type":"code","source":["tensor = torch.rand(3,6)\n","t1,t2,t3 = torch.split(tensor, 2, dim=1)\n","\n","print(tensor)\n","print(t1,t2.shape)\n","print(t2)\n","print(t3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJNMdHThapku","executionInfo":{"status":"ok","timestamp":1724805494284,"user_tz":-540,"elapsed":400,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"b543468b-786e-42b7-8e48-e3e6f08e70d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4979, 0.3808, 0.6769, 0.7992, 0.4075, 0.9803],\n","        [0.6352, 0.3078, 0.3254, 0.2530, 0.3724, 0.8114],\n","        [0.7971, 0.1244, 0.6364, 0.3231, 0.9452, 0.6802]])\n","tensor([[0.4979, 0.3808],\n","        [0.6352, 0.3078],\n","        [0.7971, 0.1244]]) torch.Size([3, 2])\n","tensor([[0.6769, 0.7992],\n","        [0.3254, 0.2530],\n","        [0.6364, 0.3231]])\n","tensor([[0.4075, 0.9803],\n","        [0.3724, 0.8114],\n","        [0.9452, 0.6802]])\n"]}]},{"cell_type":"markdown","source":["torch ↔ numpy\n","- Torch Tensor(텐서)를 NumPy array(배열)로 변환 가능\n","  - `numpy()`\n","  - `from_numpy()`\n","- Tensor가 CPU상에 있다면 NumPy 배열은 메모리 공간을 공유하므로 하나가 변하면, 다른 하나도 변함"],"metadata":{"id":"OBvgW9rPc1HE"}},{"cell_type":"code","source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyAJRW6lc1ry","executionInfo":{"status":"ok","timestamp":1724806066171,"user_tz":-540,"elapsed":390,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"5e39ef9c-1fb2-4d8c-d8b3-d98174eca8a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["a = torch.ones(7)\n","print(a)\n","print(a.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOJT5j9hc_YZ","executionInfo":{"status":"ok","timestamp":1724806090780,"user_tz":-540,"elapsed":421,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"2000b190-7e70-47cb-c3b2-2e5589e37a2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1., 1., 1.])\n","torch.float32\n"]}]},{"cell_type":"code","source":["print(a.to('cpu', torch.double)) # 텐서 'a'가 아직 CPU에 없으면 CPU로 이동. a의 데이터 유형인 torch.double로 변환\n","print(type(a))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VT1-Jy7TdxHN","executionInfo":{"status":"ok","timestamp":1724806316093,"user_tz":-540,"elapsed":398,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"842f72ab-c88a-4c87-aa1b-10ba9b41a6c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64)\n","<class 'torch.Tensor'>\n"]}]},{"cell_type":"markdown","source":["### PyTorch 데이터 타입\n","PyTorch에서 Tensor는 다양한 데이터 타입을 가질 수 있으며, 각 데이터 타입은 Tensor의 저장 및 연산 방식에 영향을 미칩니다. PyTorch에서 지원하는 주요 데이터 타입은 각기 다른 크기와 표현 방식으로 데이터를 처리하는 데 사용되며, 특히 GPU와 CPU 간의 효율적인 메모리 사용과 성능 최적화에 중요합니다.\n","\n","아래는 PyTorch에서 지원하는 주요 데이터 타입들을 정리한 표입니다.\n","\n","| 데이터 타입                | 설명                              | 약어  |\n","|----------------------------|-----------------------------------|-------|\n","| `torch.float32` (기본값)    | 32비트 부동 소수점                | `torch.float` 또는 `torch.FloatTensor` |\n","| `torch.float6..."],"metadata":{"id":"z87_Dds0ggI9"}},{"cell_type":"code","source":["b = a.numpy()\n","print(b)\n","print(type(b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTuv6ocEikS2","executionInfo":{"status":"ok","timestamp":1724807565094,"user_tz":-540,"elapsed":2,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"fede3ba3-3f18-4a21-f1fc-0a5381819f2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1. 1. 1. 1. 1.]\n","<class 'numpy.ndarray'>\n"]}]},{"cell_type":"code","source":["a.add_(1) # a값에 1을 더해줌\n","print(a)\n","print(type(a))\n","print(b)\n","print(type(b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzP7yEXXitqg","executionInfo":{"status":"ok","timestamp":1724807589850,"user_tz":-540,"elapsed":405,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"ec0fc86e-ca69-4a38-b02c-fb5f561c1100"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 2., 2., 2., 2., 2., 2.])\n","<class 'torch.Tensor'>\n","[2. 2. 2. 2. 2. 2. 2.]\n","<class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["파이토치(PyTorch)에는 데이터 작업을 위한 기본 요소 두가지인\n","torch.utils.data.DataLoader 와 torch.utils.data.Dataset 가 있습니다.\n","- Dataset 은 샘플과 정답(label)을 저장하고,\n","- DataLoader 는 Dataset 을 순회 가능한 객체(iterable)로 감쌉니다."],"metadata":{"id":"JR7mVSC6j_04"}},{"cell_type":"markdown","source":["ToTensor 는 PyTorch의 torchvision.transforms 모듈에 포함된 클래스 중 하나로, 이미지 데이터를 PyTorch에서 사용할 수 있는 Tensor로 변환하는 역할"],"metadata":{"id":"g6M3nffJkjdh"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor"],"metadata":{"id":"C1dqhHcNkAQ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 데이터셋 불러오기\n","TorchVision 에서 Fashion-MNIST 데이터셋을 불러오는 예제를 살펴보겠습니다. Fashion-MNIST는 Zalando의 기사 이미지 데이터셋으로 60,000개의 학습 예제와 10,000개의 테스트 예제로 이루어져 있습니다. 각 예제는 흑백(grayscale)의 28x28 이미지와 10개 분류(class) 중 하나인 정답(label)으로 구성됩니다.\n","\n","다음 매개변수들을 사용하여 FashionMNIST 데이터셋 을 불러옵니다:\n","- root 는 학습/테스트 데이터가 저장되는 경로입니다.\n","- train 은 학습용 또는 테스트용 데이터셋 여부를 지정합니다.\n","- download=True 는 root 에 데이터가 없는 경우 인터넷에서 다운로드합니다.\n","- transform 과 target_transform 은 특징(feature)과 정답(label) 변형(transform)을 지정합니다."],"metadata":{"id":"kVDABaYfmpQl"}},{"cell_type":"code","source":["# 공개 데이터셋에서 학습데이터를 다운\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n",")\n","\n","# 공개 데이터셋에서 테스트 데이터를 다운\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor(),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3eClhENnjrJ","executionInfo":{"status":"ok","timestamp":1724808895421,"user_tz":-540,"elapsed":8929,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"951117ea-43c3-4891-a715-49d3cdcaa7d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:04<00:00, 5744583.79it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 303376.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4422102/4422102 [00:00<00:00, 5459968.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 5113018.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]}]},{"cell_type":"markdown","source":["Dataset에 list처럼 직접 접근(index)할 수 있음. training_data[index].matplotlib을 사용하여 학습 데이터의 일부를 시각화"],"metadata":{"id":"mDpDaCCLpQrt"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","for i in range(1, cols * rows + 1): # 9개의 이미지를 위한 반복문 시작\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item() # torch.randint(100, size=(1,))은 [0,99] 범위의 랜덤 정수 하나를 생성해 (1,)크기의\n","    img, label = training_data[sample_idx]\n","    figure.add_subplot(rows,cols,i) # 현재의 서브플롯(이미지가 표시될 위치)을 3x3 르기드에서 i번째 위치로 설정\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":675},"id":"lz1_HH3epbdM","executionInfo":{"status":"ok","timestamp":1724809385361,"user_tz":-540,"elapsed":1527,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"6a885adc-b630-4deb-daf1-21f361ce711d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x800 with 9 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":[" PyTorch의 DataLoader를 사용하여 학습 데이터(training_data)와 테스트 데이터(test_data)를 배치로 로딩하고, 테스트 데이터로더에서 첫 번째 배치의 형태(shape)와 데이터 타입을 출력하는 과정\n","\n","Dataset 을 DataLoader 의 인자로 전달합니다. 이는 데이터셋을 순회 가능한 객체(iterable)로 감싸고, 자동화된 배치(batch), 샘플링(sampling),\n","섞기(shuffle) 및 다중 프로세스로 데이터 불러오기(multiprocess data loading)를 지원합니다. 여기서는 배치 크기(batch size)를 64로 정의합니다.\n","즉, 데이터로더(dataloader) 객체의 각 요소는 64개의 특징(feature)과 정답(label)을 묶음(batch)으로 반환합니다."],"metadata":{"id":"Eoruk-RfrGJV"}},{"cell_type":"code","source":["batch_size = 64\n","\n","# 데이터로더 생성\n","train_dataloader = DataLoader(training_data, batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","\n","for X, y in test_dataloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape}\") # [N, C, H,W]는 일반적으로 이미지 데이터의 차원 각각 배치크기(N), 채널수(c), 높이(H), 너비(W)를 의미\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zHLdxFLGrGg0","executionInfo":{"status":"ok","timestamp":1724815018901,"user_tz":-540,"elapsed":536,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"0af9372c-de87-422d-a26c-ededa4954364"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n","Shape of y: torch.Size([64]) torch.int64\n"]}]},{"cell_type":"markdown","source":["\n","PyTorch에서 딥러닝 모델을 구성하는 방법은 매우 직관적이며, 주요 방식은 클래스를 정의하여 모델을 설계하는 것입니다. PyTorch의 모든 모델은 torch.nn.Module 클래스를 기반으로 하며, 이 클래스를 상속하여 사용자 정의 모델을 구성할 수 있습니다. 아래는 PyTorch에서 모델을 구성하는 주요 단계와 방식에 대한 설명입니다.\n","\n","### 1. 순전파 (Forward Propagation)\n","순전파는 입력 데이터를 모델에 전달해 예측값을 계산하는 과정입니다. 이때 PyTorch는 각 연산에 대한 계산 그래프를 내부적으로 생성하고, 이 그래프를 통해 역전파를 진행할 준비를 합니다.\n","\n","```python\n","output = model(input_data)  # 순전파\n","```\n","\n","### 2. 손실 함수 계산\n","순전파 결과인 예측값을 기반으로 손실(loss)을 계산합니다. 손실 함수는 예측값과 실제값 간의 차이를 측정하는 함수입니다.\n","\n","```python\n","loss = loss_fn(output, target)  # 예측값과 실제값의 차이로 손실 계산\n","```\n","\n","### 3. 역전파 (Backward Propagation)\n","역전파는 모델의 각 매개변수에 대한 기울기(gradient)를 계산하는 과정입니다. 손실 함수에 대한 각 매개변수의 편미분 값을 구하여 가중치를 업데이트할 수 있는 방향을 결정합니다. PyTorch에서는 `loss.backward()`를 호출함으로써 자동으로 역전파가 수행됩니다.\n","\n","```python\n","loss.backward()  # 역전파 실행, 각 매개변수에 대한 기울기 계산\n","```\n","\n","이 과정을 통해 **모델의 매개변수(가중치)**에 대해 손실 함수의 기울기가 계산되며, 계산된 기울기는 `parameter.grad` 속성에 저장됩니다.\n","\n","### 4. 옵티마이저를 사용한 가중치 갱신\n","역전파를 통해 계산된 기울기를 바탕으로 옵티마이저(optimizer)가 가중치를 갱신합니다. 이를 위해 옵티마이저의 `step()` 함수를 호출합니다.\n","\n","```python\n","optimizer.step()  # 기울기를 이용해 가중치 업데이트\n","```\n","\n","### 역전파의 내부 동작\n","역전파가 어떻게 동작하는지를 자세히 설명하면 다음과 같습니다:\n","\n","1. **계산 그래프 생성**: 순전파에서 각 연산이 수행될 때, PyTorch는 각 연산을 노드로 하는 계산 그래프를 생성합니다. 이 그래프는 텐서 간의 연산을 추적하며, 역전파를 위해 각 연산에 대한 미분 방법도 저장합니다.\n","\n","2. **체인 룰 (Chain Rule)**: 역전파는 체인 룰(연쇄 법칙)을 이용하여 손실에 대한 각 매개변수의 기울기를 계산합니다. 체인 룰이란 복합 함수의 미분을 계산할 때, 각 함수의 미분을 곱하는 방법입니다. PyTorch는 이 체인 룰을 자동으로 적용해, 순전파에서 쌓인 연산들을 역순으로 따라가며 미분을 계산합니다.\n","\n","3. **기울기 저장**: 각 연산을 역으로 추적하면서, 각 매개변수에 대해 손실 함수의 기울기를 계산하고, 이를 해당 매개변수의 `.grad` 속성에 저장합니다. 이 기울기는 이후 옵티마이저에 의해 사용되어 가중치가 갱신됩니다.\n","\n","### 예시 코드\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# 간단한 선형 모델 정의\n","model = nn.Linear(2, 1)  # 입력 2, 출력 1\n","\n","# 입력과 타겟 데이터 생성\n","input_data = torch.tensor([[1.0, 2.0]], requires_grad=True)\n","target = torch.tensor([[1.0]])\n","\n","# 손실 함수와 옵티마이저 정의\n","loss_fn = nn.MSELoss()  # Mean Squared Error Loss\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# 순전파\n","output = model(input_data)\n","loss = loss_fn(output, target)\n","\n","# 역전파\n","optimizer.zero_grad()  # 기울기 초기화\n","loss.backward()        # 역전파 실행\n","optimizer.step()       # 옵티마이저를 통한 가중치 갱신\n","```\n","\n","### `requires_grad`와 계산 기록\n","PyTorch에서는 텐서의 `requires_grad` 속성을 `True`로 설정하면 해당 텐서와 관련된 모든 연산을 추적하여 자동 미분을 가능하게 합니다.\n","\n","- 텐서 `input_data`에서 `requires_grad=True`로 설정했기 때문에, 역전파 시 해당 텐서에 대한 기울기도 계산됩니다.\n","- 계산 그래프를 추적하고 싶지 않은 경우, `torch.no_grad()` 블록 안에서 연산을 수행할 수 있습니다.\n","\n","```python\n","with torch.no_grad():\n","    output = model(input_data)  # 역전파 없이 순전파만 수행\n","```\n","\n","### 중요 개념 요약\n","1. **순전파**: 입력 데이터를 모델에 전달해 출력을 계산하고, 각 연산을 추적하여 계산 그래프를 만듭니다.\n","2. **손실 계산**: 예측값과 실제값의 차이를 계산하여 손실(loss)을 구합니다.\n","3. **역전파**: 손실 함수에 대한 각 매개변수의 기울기를 계산합니다. PyTorch는 자동으로 체인 룰을 적용해 기울기를 계산합니다.\n","4. **기울기 업데이트**: 옵티마이저가 계산된 기울기를 사용하여 매개변수(가중치)를 갱신합니다.\n","\n","이 과정이 반복되면서 모델의 성능이 점차 개선되게 됩니다."],"metadata":{"id":"XVjfV8K2vmI4"}},{"cell_type":"markdown","source":["## 모델 만들기\n","PyTorch에서 신경망 모델은 nn.Module을 상속받는 클래스(class)를 생성하여 정의합니다. ``__init__`` 함수에서 신경망의 계층(layer)들을 정의하고 ``forward`` 함수에서 신경망에 데이터를 어떻게 전달할지 지정합니다. 가능한 경우 GPU로 신경망을 이동시켜 연산을 가속(accelerate)합니다."],"metadata":{"id":"zI34ZFz5zkz8"}},{"cell_type":"code","source":["# 학습에 사용할 CPU나 GPU, MPS 장치를 생성\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"cpu\"\n",")\n","\n","print(f'Using {device} device')\n","\n","# 모델 정의\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512,512),\n","            nn.ReLU(),\n","            nn.Linear(512,512),\n","            nn.ReLU(),\n","            nn.Linear(512,10)\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork().to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzQtIltp1gAp","executionInfo":{"status":"ok","timestamp":1724815023297,"user_tz":-540,"elapsed":390,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"b51d7c9e-c17b-481f-b5e3-97b2a912c343"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=512, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["#### 학습을 위한 장치 얻기\n","가능한 경우 GPU와 같은 하드웨어 가속기에서 모델을 학습하려고 합니다. torch.cuda가 사용 가능한지 확인해보고, 그렇지 않으면 CPU를 계속 사용합니다.\n","\n","#### 클래스 정의하기\n","신경망 모델을 nn.Module 의 하위클래스로 정의하고, __init__ 에서 신경망 계층들을 초기화합니다. nn.Module 을 상속받은 모든 클래스는 forward 메소드에 입력 데이터에 대한 연산들을 구현합니다."],"metadata":{"id":"HlJJJs2p4RUe"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # 각 레이어를 개별적으로 정의\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(28*28, 512)    # 첫 번째 선형 계층\n","        self.relu1 = nn.ReLU()              # 첫 번째 ReLU 활성화 함수\n","        self.fc2 = nn.Linear(512, 512)      # 두 번째 선형 계층\n","        self.relu2 = nn.ReLU()              # 두 번째 ReLU 활성화 함수\n","        self.fc3 = nn.Linear(512, 10)       # 세 번째 선형 계층 (출력 레이어)\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","# 모델을 정의하고 장치에 할당\n","device = torch.device(\"cude\" if torch.cuda.is_available() else \"cpu\")\n","model = NeuralNetwork().to(device)\n","\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7yACNmh4O3E","executionInfo":{"status":"ok","timestamp":1724815026043,"user_tz":-540,"elapsed":395,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"fbc8869b-e5f2-4e4f-8a24-ab7d458d28f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fc1): Linear(in_features=784, out_features=512, bias=True)\n","  (relu1): ReLU()\n","  (fc2): Linear(in_features=512, out_features=512, bias=True)\n","  (relu2): ReLU()\n","  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["각 학습단계(training loop)에서 모델은 (배치(batch)로 제공되는) 학습 데이터셋에 대한 예측을 수행하고, 예측 오류를 역전파하여 모델의 매개변수를 조정함."],"metadata":{"id":"DrxenNpH89qp"}},{"cell_type":"code","source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        X, y = X.to(device), y.to(device)\n","\n","        # 예측 오류 계산\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # 역전파\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), (batch + 1) * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"],"metadata":{"id":"Y8F-_2vL9JgC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델이 학습하고 있는지를 확인하기 위해 테스트 데이터셋으로 모델의 성능을 확인합니다."],"metadata":{"id":"GWicPS-E9afH"}},{"cell_type":"code","source":["def test(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"Cd1ciYYq99mC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["학습 단계는 여러번의 반복 단계 (에폭(epochs))를 거쳐서 수행됨. 각 에폭에서는 모델은 더 나은 예측을 하기위해 매개변수를 학습함. 각 에폭마다 모델의 정확도(accuracy)와 손실(loss)를 출력함. 에폭마다 정확도는 증가하고 손실이 감소하는걸 확인하기 위함."],"metadata":{"id":"Zmm0LNsY-pR4"}},{"cell_type":"code","source":["epochs = 5\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_dataloader, model, loss_fn, optimizer)\n","    test(test_dataloader, model, loss_fn)\n","print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"P-_1kW_A-3Xj","executionInfo":{"status":"error","timestamp":1724815031239,"user_tz":-540,"elapsed":469,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"82266a66-2683-45d9-dbbe-6c4e7e05420c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'loss_fn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-c0bb85f41759>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'loss_fn' is not defined"]}]},{"cell_type":"markdown","source":["## 모델 저장하기\n","모델을 저장하는 일반적인 방법은 (모델의 매개변수들을 포함하여) 내부 상태 사전(internal state dictionary)을\n","직렬화(serialize)하는 것입니다.\n","\n","\n","## 모델 불러오기\n","\n","모델을 불러오는 과정에는 모델 구조를 다시 만들고 상태 사전을 모델에 불러오는 과정이 포함됩니다."],"metadata":{"id":"_UZ0piV-WNMd"}},{"cell_type":"markdown","source":["- torch: 이것은 메인 PyTorch 라이브러리입니다. 여기에는 GPU를 통한 가속을 통한 텐서 계산 지원, 신경망 훈련을 용이하게 하는 자동 차별화, 모델 구축 및 훈련을 위한 다양한 유틸리티가 포함됩니다.\n","- torch.nn: 레이어, 활성화 함수, 손실 함수와 같은 신경망의 구성 요소를 제공하는 PyTorch의 하위 모듈입니다. 신경망의 아키텍처를 정의하는 데 필수적입니다.\n","- torch.nn.function: 이 모듈에는 torch.nn 레이어에서 사용되는 기능이 포함되어 있습니다. 입력 데이터 및 가중치에 이러한 함수를 직접 사용할 수 있으므로 일부 작업에 더 많은 유연성을 제공합니다. 여기에는 활성화, 손실 계산 및 상태(즉, 가중치)를 유지하지 않는 다양한 기타 작업을 위한 함수가 포함됩니다.\n","- torch.optim: 이 하위 모듈은 SGD(Stochastic Gradient Descent), Adam 등과 같은 신경망 훈련을 위한 최적화 알고리즘을 제공합니다. 이러한 최적화 프로그램은 계산된 기울기를 기반으로 네트워크의 가중치를 업데이트하는 데 사용됩니다.\n","- torchvision: 이미지 데이터 작업을 위한 유틸리티를 제공하는 PyTorch 프로젝트의 패키지입니다. 여기에는 사전 정의된 데이터세트(예: MNIST, CIFAR10, FashionMNIST), 모델 아키텍처(예: ResNet, AlexNet) 및 전처리를 위한 일반적인 이미지 변환이 포함됩니다.\n","- torchvision.transforms: 일반적인 이미지 변환을 제공하는 torchvision 내의 모듈입니다. 이는 이미지를 신경망에 공급하기 전에 데이터 증대 및 이미지 전처리에 사용될 수 있습니다. 예로는 크기 조정, 정규화, 텐서로 변환 등이 있습니다.\n","- SubsetRandomSampler: 교체 없이 데이터세트에서 요소를 무작위로 샘플링하는 데 사용되는 도구입니다. 데이터 세트를 훈련 및 검증/테스트 세트로 분할하거나 모델 훈련을 위해 사용자 정의 데이터 샘플링 전략을 구현하려는 경우에 특히 유용"],"metadata":{"id":"nTeEYlDkWPfM"}},{"cell_type":"code","source":["import torch\n","from torch.untils.data import DataLoader\n","\n","# MNIST 데이터셋을 위한 전처리 과정 정의\n","transform = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(), # 이미지를 pytorch 텐서로 변환\n","    torchvision.transforms.Normalize((0.5,), (0.5,)) # 이미지 평균 0.5, 표준편차 0.5로 정규화 [-1, 1] 범위로 조정.\n","])\n","\n","# MINIST 데이터셋 로드\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# 훈련 데이터셋을 훈련 . 검증 셋으로 분할\n","train_size = int(0.8 * len(train_dataset)) # 훈련 세트 크기를 전체의 80% 설정\n","val_size = len(train_dataset) - train_size # 검증 세트 크기계산\n","train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size]) # 분할 실행\n","\n","# 데이터 로더 생성\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# 모델 아키텍처 정의\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 20, 5) # 컨볼루션 레이어 정의 (입력 1, 출력 20, 커널크기 5)\n","        self.pool = nn.MaxPool2d(2, 2) # 맥스풀링 레이어 정의 2x2 풀링\n","        self.flattten() # 텐서 평탄화\n","        self.fc1 = nn.Linear(2880, 50) # 완전 연결 레이어 (입력 크기 2880, 출력크기 50)\n","        self.fc2 = nn.Linear(50, 10) # 출력 레이어 클래스수 10\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) # ReLu 활성화 함수 적용 후 맥스풀링\n","        x = self.flatten(x) # 평탄화\n","        x = F.relu(self.fc1(x)) # ReLu 활성화 함수 적용\n","        x = self.fc2(x) # 최종 출력\n","        return x\n","\n","model = MyModel() # 모델 인스턴스 생성\n","\n","# 손실 함수 및 최적화 알고리즘 지정\n","criterion = nn.CrossEntropyLoss() # 교차 엔트로피 손실 함수\n","optimizer = optim.Adam(model.parameters(), lr=0.01, momentum=0.9) # SGD 최적화 알고리즘\n","\n","# 모델 훈련\n","best_val_loss = float('inf') # 검증 손실을 추적하기 위한 변수 초기화\n","patience, trials = 5, 0 # 조기 종료 기준 설정\n","num_epochs = 20 # 에폭 수 설정\n","for epoch in range(num_epochs):\n","    model.train() # 모델을 훈련 모드 설정\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad() # 기울기 초기화\n","        outputs = model(inputs) # 모델을 통한 순전파\n","        loss = criterion(outputs, labels) # 손실 계산\n","        loss.backward() # 역전파\n","        optimizer.step() # 가중치 업데이트\n","        running_loss += loss.item()\n","\n","    # 검증 단계\n","    val_loss = 0.0\n","    model.eval() # 모델을 평가 모드로 설정\n","    with torch.no_grad():\n","        for inputs, labels in valloader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","    val_loss /= len(valloader) # 평균 검증 손실 계산\n","    print(f'Epoch {epoch+1}, Train Loss: {running_loss / len(trainloader)}, Val Loss: {val_loss}')\n","\n","    # 검증 손실이 개선되었는지 확인하고 모델 저장\n","    if val_loss < best_val_loss:\n","        print(f'Validation Loss Decreased({best_val_loss:.6f}--->{val_loss:.6f}) \\t Saving Model')\n","        best_val_loss = val_loss\n","        trials = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        trials += 1\n","        if trials >= patience: # 조기 종료 조건 충족 확인\n","            print(f'Early stopping triggered')\n","            break\n","\n","# 최고의 모델을 불러와서 평가\n","model.load_state_dict(torch.load('best_model.pth')) # 모델 상태 불러오기\n","\n","# 모델평가\n","correct = 0\n","total = 0\n","with torch.no_grad():  # 그래디언트 계산 비활성화\n","    for inputs, labels in testloader:\n","        outputs = model(inputs)\n","        _.predicted = torch.max(outputs, 1) # torch.max(input, dim), 'dim'은 차원을 의미, 클래스 점수들 중에서 최대값을 찾는다는 으미ㅣ\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","accuracy = 100 * correct / total # 정확도 계산\n","print(f'Test Accuracy: {accuracy:.2f}%') # 정확도 출력\n"],"metadata":{"id":"MzHcA6vMstxp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- torch: 이것은 메인 PyTorch 라이브러리입니다. 여기에는 GPU를 통한 가속 텐서 계산 지원, 신경망 훈련을 용이하게 하는 자동 차별화, 모델 구축 및 훈련을 위한 다양한 유틸리티가 포함됩니다.\n","- torch.nn: 레이어, 활성화 함수, 손실 함수와 같은 신경망의 구성 요소를 제공하는 PyTorch의 하위 모듈입니다. 신경망의 아키텍처를 정의하는 데 필수적입니다.\n","- torch.nn.function: 이 모듈에는 torch.nn 레이어에서 사용되는 기능이 포함되어 있습니다. 입력 데이터 및 가중치에 이러한 함수를 직접 사용할 수 있으므로 일부 작업에 더 많은 유연성을 제공합니다. 여기에는 활성화, 손실 계산 및 상태(즉, 가중치)를 유지하지 않는 다양한 기타 작업을 위한 함수가 포함됩니다.\n","- torch.optim: 이 하위 모듈은 SGD(Stochastic Gradient Descent), Adam 등과 같은 신경망 훈련을 위한 최적화 알고리즘을 제공합니다. 이러한 최적화 프로그램은 계산된 기울기를 기반으로 네트워크의 가중치를 업데이트하는 데 사용됩니다.\n","- torchvision: 이미지 데이터 작업을 위한 유틸리티를 제공하는 PyTorch 프로젝트의 패키지입니다. 여기에는 사전 정의된 데이터세트(예: MNIST, CIFAR10, FashionMNIST), 모델 아키텍처(예: ResNet, AlexNet) 및 전처리를 위한 일반적인 이미지 변환이 포함됩니다.\n","- torchvision.transforms: 일반적인 이미지 변환을 제공하는 torchvision 내의 모듈입니다. 이는 이미지를 신경망에 공급하기 전에 데이터 증대 및 이미지 전처리에 사용될 수 있습니다. 예로는 크기 조정, 정규화, 텐서로 변환 등이 있습니다.\n","- SubsetRandomSampler: 교체 없이 데이터세트에서 요소를 무작위로 샘플링하는 데 사용되는 도구입니다. 데이터 세트를 훈련 및 검증/테스트 세트로 분할하거나 모델 훈련을 위해 사용자 정의 데이터 샘플링 전략을 구현하려는 경우에 특히 유용"],"metadata":{"id":"D3G5hJ850Ku6"}},{"cell_type":"markdown","source":["Q. PyTorch를 사용하여 FashionMNIST 데이터세트에 대한 분류 모델링 및 평가를 다음과 같은 단계로 수행하세요.\n","- 1단계: 신경망 모델 정의\n","- 2단계: FashionMNIST 데이터셋 로드\n","- 3단계: 네트워크, 손실 함수, 최적화 알고리즘 초기화\n","- 4단계: 조기 종료를 포함한 모델 학습 및 best model 저장\n","- 5단계: best model을 로드하고 테스트 데이터셋으로 평가"],"metadata":{"id":"ItHqg_v10MyO"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# 1단계: 신경망 모델 정의\n","# Net 클래스는 nn.Module을 상속받아 만들어진 사용자 정의 신경망 모델로, FashionMNIST 데이터셋의 이미지 분류를 위해 설계\n","class Net(nn.Module):\n","# 모델의 구조를 정의합니다. 이 모델은 세 개의 완전 연결(fully-connected) 레이어를 포함\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 120)  # 입력으로 28x28 크기의 이미지를 받아 펼친 후(784개의 피처), 120개의 출력 노드로 연결\n","        self.fc2 = nn.Linear(120, 84) # 120개의 입력 노드에서 84개의 출력 노드로 연결\n","        self.fc3 = nn.Linear(84, 10)  # 84개의 입력 노드에서 최종적으로 10개의 출력 노드로 연결\n","# forward 메소드는 신경망에 데이터를 어떻게 전달할지 정의\n","    def forward(self, x):\n","        x = x.view(-1, 28 * 28) # 입력 이미지 x를 1차원으로 펼치는 과정입니다. -1은 배치 크기에 대응되며, 이는 어떤 크기의 배치라도 처리할 수 있음을 의미\n","        x = F.relu(self.fc1(x)) # 첫 번째 레이어를 통과한 후, ReLU 활성화 함수를 적용합니다. 이는 비선형성을 도입하여 모델이 더 복잡한 패턴을 학습\n","        x = F.relu(self.fc2(x)) # 두 번째 레이어를 통과한 후, 다시 ReLU 활성화 함수를 적용\n","        x = self.fc3(x) # 세 번째 레이어를 통과한 결과를 반환. 소프트맥스 함수 등을 통해 확률로 변환되어 클래스를 예측하는 데 사용\n","        return x\n","\n","# 2단계: FashionMNIST 데이터셋 로드\n","# torchvision 라이브러리를 사용하여 FashionMNIST 데이터셋을 다운로드하고, 데이터를 전처리하기 위한 변환(transform)을 설정하는 과정\n","\n","# transforms.Compose는 여러 전처리 단계를 하나로 묶어주는 역할\n","# transforms.ToTensor(): 이미지를 PyTorch 텐서로 변환. 이미지의 픽셀 값 범위가 0에서 255 사이의 정수에서 0.0에서 1.0 사이의 부동소수점으로 변경\n","# 모든 채널의 평균을 0.5로, 표준편차를 0.5로 설정합니다. 이는 데이터의 범위를 대략적으로 -1.0에서 1.0 사이로 조정\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","# FashionMNIST 데이터셋을 다운로드하고, 지정된 변환을 적용하여 데이터를 준비하는 함수\n","train_val_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# 훈련 및 검증 분할을 위한 데이터 인덱스 생성\n","dataset_size = len(train_val_dataset) # 훈련 및 검증 데이터셋의 전체 크기\n","indices = list(range(dataset_size)) # 데이터셋 내의 모든 샘플에 대한 인덱스를 포함\n","validation_split = 0.1 # 검증 세트로 사용될 데이터의 비율\n","split = int(np.floor(validation_split * dataset_size)) # 검증 세트의 크기를 계산\n","np.random.shuffle(indices) # 훈련 및 검증 세트가 데이터셋의 특정 부분에 치우치지 않도록 하기 위함\n","train_indices, val_indices = indices[split:], indices[:split] # 섞인 인덱스를 사용하여 훈련 세트와 검증 세트의 인덱스를 분할\n","\n","# PT 데이터 샘플러 및 로더 생성\n","# 훈련 세트와 검증 세트에 대한 데이터 로더를 설정하고, 테스트 세트에 대한 데이터 로더를 별도로 설정하는 과정입니다.\n","# 이러한 데이터 로더들은 모델 학습, 검증, 테스트 과정에서 배치 단위로 데이터를 로드하는 데 사용\n","train_sampler = SubsetRandomSampler(train_indices) # train_indices에 해당하는 훈련 데이터의 인덱스를 무작위로 샘플링하는 샘플러를 생성\n","val_sampler = SubsetRandomSampler(val_indices) # val_indices에 해당하는 검증 데이터의 인덱스로부터 데이터를 무작위로 샘플링하는 샘플러를 생성\n","# 배치 크기 4로 로드하는 훈련 데이터 로더를 생성(# 데이터의 무작위 샘플링을 수행)\n","trainloader = torch.utils.data.DataLoader(train_val_dataset, batch_size=4, sampler=train_sampler)\n","valloader = torch.utils.data.DataLoader(train_val_dataset, batch_size=4, sampler=val_sampler)\n","# shuffle=False 인자를 통해 셔플링 없이 순서대로 데이터를 로드. 테스트 과정에서는 데이터의 순서가 결과에 영향을 미치지 않으므로 셔플링을 수행하지 않습니다.\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n","\n","# 3단계: 네트워크, 손실 함수, 최적화 알고리즘 초기화\n","net = Net() #  클래스의 인스턴스를 생성하여 net 변수에 할당\n","criterion = nn.CrossEntropyLoss() # 멀티클래스 분류 문제에서 널리 사용되는 손실 함수로, 모델의 예측값과 실제 타겟값 사이의 차이를 측정\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 모멘텀은 최적화 과정에서 이전 그래디언트의 방향을 고려, 파라미터 업데이트 시 관성을 부여\n","\n","# 4단계: 조기 종료를 포함한 모델 학습\n","patience = 5 # 검증 손실이 개선되지 않을 때, 훈련을 계속 진행하기 전에 기다릴 에폭 수를 의미\n","patience_counter = 0\n","best_val_loss = np.Inf # 최고의 검증 손실 값을 무한대로 초기화. 훈련 과정에서 검증 손실이 이전에 기록된 최소 손실보다 낮아지면 업데이트되는 값\n","\n","# 모델을 에폭 단위로 반복 훈련시키면서, 각 배치의 손실을 계산하고 모델 파라미터를 업데이트하는 기본적인 훈련 과정을 구현\n","for epoch in range(20):  # 데이터셋을 여러 번 반복\n","    net.train()  # 모델을 학습 모드로 설정. 이는 모델 내의 특정 레이어(예: 드롭아웃, 배치 정규화 등)가 훈련 시와 평가 시 다르게 동작해야 할 때 필요\n","    running_loss = 0.0 # 현재 에폭의 총 손실을 계산하기 위해 실행 손실을 0으로 초기화\n","    for i, data in enumerate(trainloader, 0): # '0'은 열거의 시작 인덱스를 지정\n","        inputs, labels = data\n","        optimizer.zero_grad() # 최적화를 수행하기 전에 모델의 그래디언트를 0으로 초기화\n","        outputs = net(inputs) # 현재 배치의 입력 데이터를 모델에 전달하여 예측값을 계산\n","        loss = criterion(outputs, labels) # 모델의 예측값과 실제 레이블 간의 손실을 계산\n","        loss.backward() # 손실 함수의 그래디언트를 역전파합니다. 이 과정에서 모델 파라미터에 대한 손실의 미분값이 계산\n","        optimizer.step() # 계산된 그래디언트를 사용하여 모델의 파라미터를 업데이트\n","        running_loss += loss.item() # 현재 배치의 손실을 실행 손실에 누적. 이를 통해 전체 에폭의 평균 손실을 계산\n","\n","    # 검증 단계\n","    net.eval()  # 모델을 평가 모드로 설정\n","    val_loss = 0.0 # 검증 손실을 계산하기 위한 변수를 0으로 초기화\n","    with torch.no_grad(): # 이 블록 내에서는 그래디언트 계산을 비활성화. 평가 모드에서는 모델을 업데이트하지 않으므로, 그래디언트를 계산할 필요가 없습니다\n","        for inputs, labels in valloader: # 검증 데이터 로더(valloader)에서 배치 단위로 데이터를 로드하여 반복\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels) # criterion은 손실 함수로, 모델의 성능을 측정하는 기준\n","            val_loss += loss.item() # 누적된 검증 손실을 검증 데이터 배치의 총 수로 나누어 평균 검증 손실을 계산\n","    running_loss /= len(trainloader)\n","    val_loss /= len(valloader)\n","    print(f'에폭 {epoch + 1}, 훈련 손실: {running_loss}, 검증 손실: {val_loss}')\n","\n","    # 조기 종료 체크\n","    if val_loss < best_val_loss: # 현재 에폭에서 계산된 검증 손실(val_loss)이 이전에 기록된 최소 검증 손실(best_val_loss)보다 낮은지 확인\n","        print(f'검증 손실이 감소하였습니다. ({best_val_loss:.6f} 에서 {val_loss:.6f}로). 모델 저장 중...')\n","        torch.save(net.state_dict(), '/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model.pth')\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print('조기 종료 발동.')\n","            break\n","\n","# 5단계: 최고의 모델을 로드하고 테스트 데이터셋으로 평가\n","# torch.load 함수는 지정된 경로에서 모델 파라미터를 불러오며, load_state_dict 메서드를 사용하여 이 파라미터를 현재 net 모델에 로드\n","net.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model.pth'))\n","correct = 0 # 정확히 분류된 샘플의 수를 세기 위한 변수\n","total = 0 # 테스트셋의 전체 샘플 수를 세기 위한 변수\n","with torch.no_grad():\n","    for data in testloader: # 테스트 데이터셋을 배치 단위로 순회\n","        images, labels = data\n","        outputs = net(images) # 현재 배치의 이미지를 모델에 전달하여 예측값을 계산\n","        _, predicted = torch.max(outputs.data, 1) # torch.max는 각 예측에 대한 최대값과 그 위치(인덱스)를 반환. 위치만 필요하므로 _를 사용하여 최대값은 무시\n","        total += labels.size(0) # labels.size(0)는 현재 배치의 크기(샘플 수)\n","        correct += (predicted == labels).sum().item() # 일치하는 경우의 수를 텐서 형태로 반환하며, .item()으로 이를 파이썬의 스칼라 값으로 변환\n","\n","print(f'10000개의 테스트 이미지에 대한 네트워크의 정확도: {100 * correct / total} %')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8QzsyvX0wjs","executionInfo":{"status":"ok","timestamp":1724902551329,"user_tz":-540,"elapsed":577088,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"0111afd2-81e1-4c8e-c2a3-e001035bef56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["에폭 1, 훈련 손실: 0.5355455617888624, 검증 손실: 0.4590335133126937\n","검증 손실이 감소하였습니다. (inf 에서 0.459034로). 모델 저장 중...\n","에폭 2, 훈련 손실: 0.3925418913984807, 검증 손실: 0.38262253552876063\n","검증 손실이 감소하였습니다. (0.459034 에서 0.382623로). 모델 저장 중...\n","에폭 3, 훈련 손실: 0.35175505601733137, 검증 손실: 0.3666762349816466\n","검증 손실이 감소하였습니다. (0.382623 에서 0.366676로). 모델 저장 중...\n","에폭 4, 훈련 손실: 0.32520426057713675, 검증 손실: 0.3429506902969224\n","검증 손실이 감소하였습니다. (0.366676 에서 0.342951로). 모델 저장 중...\n","에폭 5, 훈련 손실: 0.30658242243310085, 검증 손실: 0.33951377120363757\n","검증 손실이 감소하였습니다. (0.342951 에서 0.339514로). 모델 저장 중...\n","에폭 6, 훈련 손실: 0.29121835178955174, 검증 손실: 0.3156059864499421\n","검증 손실이 감소하였습니다. (0.339514 에서 0.315606로). 모델 저장 중...\n","에폭 7, 훈련 손실: 0.27792548505678005, 검증 손실: 0.3395249951972598\n","에폭 8, 훈련 손실: 0.26643992240862474, 검증 손실: 0.3074835789194288\n","검증 손실이 감소하였습니다. (0.315606 에서 0.307484로). 모델 저장 중...\n","에폭 9, 훈련 손실: 0.25673874120225354, 검증 손실: 0.30468444560589536\n","검증 손실이 감소하였습니다. (0.307484 에서 0.304684로). 모델 저장 중...\n","에폭 10, 훈련 손실: 0.24708835996281595, 검증 손실: 0.33952068893690435\n","에폭 11, 훈련 손실: 0.23589263980793718, 검증 손실: 0.32864315318405424\n","에폭 12, 훈련 손실: 0.23019530897036872, 검증 손실: 0.31885224884473085\n","에폭 13, 훈련 손실: 0.22136693563566, 검증 손실: 0.31661890871484094\n","에폭 14, 훈련 손실: 0.21541960380473804, 검증 손실: 0.3295126383911152\n","조기 종료 발동.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-6d6d736ab7ca>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model.pth'))\n"]},{"output_type":"stream","name":"stdout","text":["10000개의 테스트 이미지에 대한 네트워크의 정확도: 88.24 %\n"]}]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# 1단계: 신경망 모델 정의\n","# Net 클래스는 nn.Module을 상속받아 만들어진 사용자 정의 신경망 모델로, FashionMNIST 데이터셋의 이미지 분류를 위해 설계\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 첫 번째 합성곱 레이어: 입력 채널 1개, 출력 채널 6개, 커널 크기 5x5\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        # 두 번째 합성곱 레이어: 입력 채널 6개, 출력 채널 16개, 커널 크기 5x5\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # 전결합 레이어\n","        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 16개의 채널과 4x4 이미지 크기\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # 첫 번째 합성곱, ReLU 활성화, 맥스 풀링\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        # 두 번째 합성곱, ReLU 활성화, 맥스 풀링\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        # 입력 데이터를 1차원으로 펼침\n","        x = x.view(-1, 16 * 4 * 4)\n","        # 전결합 레이어와 ReLU 활성화 함수 적용\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        # 마지막 레이어 출력\n","        x = self.fc3(x)\n","        return x\n","\n","\n","# class Net(nn.Module):\n","#     def __init__(self):\n","#         super(Net, self).__init__()\n","#         # 첫 번째 합성곱 레이어: 입력 채널 1개, 출력 채널 6개, 커널 크기 5x5\n","#         self.conv1 = nn.Conv2d(1, 6, 5)\n","#         # 두 번째 합성곱 레이어: 입력 채널 6개, 출력 채널 16개, 커널 크기 5x5\n","#         self.conv2 = nn.Conv2d(6, 16, 5)\n","#         self.pool = nn.MaxPool2d(2, 2)\n","#         # 전결합 레이어\n","#         self.fc1 = nn.Linear(16 * 4 * 4, 120)  # 16개의 채널과 4x4 이미지 크기\n","#         self.fc2 = nn.Linear(120, 84)\n","#         self.fc3 = nn.Linear(84, 10)\n","\n","#     def forward(self, x):\n","#         # 첫 번째 합성곱, ReLU 활성화, 맥스 풀링\n","#         x = self.pool(F.relu(self.conv1(x)))\n","#         # 두 번째 합성곱, ReLU 활성화, 맥스 풀링\n","#         x = self.pool(F.relu(self.conv2(x)))\n","#         # 입력 데이터를 1차원으로 펼침\n","#         x = self.flatten(x)\n","#         # x = x.view(-1, 16 * 4 * 4)\n","#         # 전결합 레이어와 ReLU 활성화 함수 적용\n","#         x = F.relu(self.fc1(x))\n","#         x = F.relu(self.fc2(x))\n","#         # 마지막 레이어 출력\n","#         x = self.fc3(x)\n","#         return x\n","\n","# 2단계: FashionMNIST 데이터셋 로드\n","# torchvision 라이브러리를 사용하여 FashionMNIST 데이터셋을 다운로드하고, 데이터를 전처리하기 위한 변환(transform)을 설정하는 과정\n","\n","# transforms.Compose는 여러 전처리 단계를 하나로 묶어주는 역할\n","# transforms.ToTensor(): 이미지를 PyTorch 텐서로 변환. 이미지의 픽셀 값 범위가 0에서 255 사이의 정수에서 0.0에서 1.0 사이의 부동소수점으로 변경\n","# 모든 채널의 평균을 0.5로, 표준편차를 0.5로 설정합니다. 이는 데이터의 범위를 대략적으로 -1.0에서 1.0 사이로 조정\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","# FashionMNIST 데이터셋을 다운로드하고, 지정된 변환을 적용하여 데이터를 준비하는 함수\n","train_val_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# 훈련 및 검증 분할을 위한 데이터 인덱스 생성\n","dataset_size = len(train_val_dataset) # 훈련 및 검증 데이터셋의 전체 크기\n","indices = list(range(dataset_size)) # 데이터셋 내의 모든 샘플에 대한 인덱스를 포함\n","validation_split = 0.1 # 검증 세트로 사용될 데이터의 비율\n","split = int(np.floor(validation_split * dataset_size)) # 검증 세트의 크기를 계산\n","np.random.shuffle(indices) # 훈련 및 검증 세트가 데이터셋의 특정 부분에 치우치지 않도록 하기 위함\n","train_indices, val_indices = indices[split:], indices[:split] # 섞인 인덱스를 사용하여 훈련 세트와 검증 세트의 인덱스를 분할\n","\n","# PT 데이터 샘플러 및 로더 생성\n","# 훈련 세트와 검증 세트에 대한 데이터 로더를 설정하고, 테스트 세트에 대한 데이터 로더를 별도로 설정하는 과정입니다.\n","# 이러한 데이터 로더들은 모델 학습, 검증, 테스트 과정에서 배치 단위로 데이터를 로드하는 데 사용\n","train_sampler = SubsetRandomSampler(train_indices) # train_indices에 해당하는 훈련 데이터의 인덱스를 무작위로 샘플링하는 샘플러를 생성\n","val_sampler = SubsetRandomSampler(val_indices) # val_indices에 해당하는 검증 데이터의 인덱스로부터 데이터를 무작위로 샘플링하는 샘플러를 생성\n","# 배치 크기 4로 로드하는 훈련 데이터 로더를 생성(# 데이터의 무작위 샘플링을 수행)\n","trainloader = torch.utils.data.DataLoader(train_val_dataset, batch_size=4, sampler=train_sampler)\n","valloader = torch.utils.data.DataLoader(train_val_dataset, batch_size=4, sampler=val_sampler)\n","# shuffle=False 인자를 통해 셔플링 없이 순서대로 데이터를 로드. 테스트 과정에서는 데이터의 순서가 결과에 영향을 미치지 않으므로 셔플링을 수행하지 않습니다.\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n","\n","# 3단계: 네트워크, 손실 함수, 최적화 알고리즘 초기화\n","net = Net() #  클래스의 인스턴스를 생성하여 net 변수에 할당\n","criterion = nn.CrossEntropyLoss() # 멀티클래스 분류 문제에서 널리 사용되는 손실 함수로, 모델의 예측값과 실제 타겟값 사이의 차이를 측정\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # 모멘텀은 최적화 과정에서 이전 그래디언트의 방향을 고려, 파라미터 업데이트 시 관성을 부여\n","\n","# 4단계: 조기 종료를 포함한 모델 학습\n","patience = 5 # 검증 손실이 개선되지 않을 때, 훈련을 계속 진행하기 전에 기다릴 에폭 수를 의미\n","patience_counter = 0\n","best_val_loss = np.Inf # 최고의 검증 손실 값을 무한대로 초기화. 훈련 과정에서 검증 손실이 이전에 기록된 최소 손실보다 낮아지면 업데이트되는 값\n","\n","# 모델을 에폭 단위로 반복 훈련시키면서, 각 배치의 손실을 계산하고 모델 파라미터를 업데이트하는 기본적인 훈련 과정을 구현\n","for epoch in range(20):  # 데이터셋을 여러 번 반복\n","    net.train()  # 모델을 학습 모드로 설정. 이는 모델 내의 특정 레이어(예: 드롭아웃, 배치 정규화 등)가 훈련 시와 평가 시 다르게 동작해야 할 때 필요\n","    running_loss = 0.0 # 현재 에폭의 총 손실을 계산하기 위해 실행 손실을 0으로 초기화\n","    for i, data in enumerate(trainloader, 0): # '0'은 열거의 시작 인덱스를 지정\n","        inputs, labels = data\n","        optimizer.zero_grad() # 최적화를 수행하기 전에 모델의 그래디언트를 0으로 초기화\n","        outputs = net(inputs) # 현재 배치의 입력 데이터를 모델에 전달하여 예측값을 계산\n","        loss = criterion(outputs, labels) # 모델의 예측값과 실제 레이블 간의 손실을 계산\n","        loss.backward() # 손실 함수의 그래디언트를 역전파합니다. 이 과정에서 모델 파라미터에 대한 손실의 미분값이 계산\n","        optimizer.step() # 계산된 그래디언트를 사용하여 모델의 파라미터를 업데이트\n","        running_loss += loss.item() # 현재 배치의 손실을 실행 손실에 누적. 이를 통해 전체 에폭의 평균 손실을 계산\n","\n","    # 검증 단계\n","    net.eval()  # 모델을 평가 모드로 설정\n","    val_loss = 0.0 # 검증 손실을 계산하기 위한 변수를 0으로 초기화\n","    with torch.no_grad(): # 이 블록 내에서는 그래디언트 계산을 비활성화. 평가 모드에서는 모델을 업데이트하지 않으므로, 그래디언트를 계산할 필요가 없습니다\n","        for inputs, labels in valloader: # 검증 데이터 로더(valloader)에서 배치 단위로 데이터를 로드하여 반복\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels) # criterion은 손실 함수로, 모델의 성능을 측정하는 기준\n","            val_loss += loss.item() # 누적된 검증 손실을 검증 데이터 배치의 총 수로 나누어 평균 검증 손실을 계산\n","    running_loss /= len(trainloader)\n","    val_loss /= len(valloader)\n","    print(f'에폭 {epoch + 1}, 훈련 손실: {running_loss}, 검증 손실: {val_loss}')\n","\n","    # 조기 종료 체크\n","    if val_loss < best_val_loss: # 현재 에폭에서 계산된 검증 손실(val_loss)이 이전에 기록된 최소 검증 손실(best_val_loss)보다 낮은지 확인\n","        print(f'검증 손실이 감소하였습니다. ({best_val_loss:.6f} 에서 {val_loss:.6f}로). 모델 저장 중...')\n","        torch.save(net.state_dict(), '/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model.pth')\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print('조기 종료 발동.')\n","            break\n","\n","# 5단계: 최고의 모델을 로드하고 테스트 데이터셋으로 평가\n","# torch.load 함수는 지정된 경로에서 모델 파라미터를 불러오며, load_state_dict 메서드를 사용하여 이 파라미터를 현재 net 모델에 로드\n","net.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model.pth'))\n","correct = 0 # 정확히 분류된 샘플의 수를 세기 위한 변수\n","total = 0 # 테스트셋의 전체 샘플 수를 세기 위한 변수\n","with torch.no_grad():\n","    for data in testloader: # 테스트 데이터셋을 배치 단위로 순회\n","        images, labels = data\n","        outputs = net(images) # 현재 배치의 이미지를 모델에 전달하여 예측값을 계산\n","        _, predicted = torch.max(outputs.data, 1) # torch.max는 각 예측에 대한 최대값과 그 위치(인덱스)를 반환. 위치만 필요하므로 _를 사용하여 최대값은 무시\n","        total += labels.size(0) # labels.size(0)는 현재 배치의 크기(샘플 수)\n","        correct += (predicted == labels).sum().item() # 일치하는 경우의 수를 텐서 형태로 반환하며, .item()으로 이를 파이썬의 스칼라 값으로 변환\n","\n","print(f'10000개의 테스트 이미지에 대한 네트워크의 정확도: {100 * correct / total} %')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bO0WJYeNMe-","executionInfo":{"status":"ok","timestamp":1724903496896,"user_tz":-540,"elapsed":890018,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"269ad504-141e-4dc6-8b82-82a290885f9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["에폭 1, 훈련 손실: 0.6300057952790219, 검증 손실: 0.4562278207014315\n","검증 손실이 감소하였습니다. (inf 에서 0.456228로). 모델 저장 중...\n","에폭 2, 훈련 손실: 0.3809727824984848, 검증 손실: 0.3513052126133892\n","검증 손실이 감소하였습니다. (0.456228 에서 0.351305로). 모델 저장 중...\n","에폭 3, 훈련 손실: 0.32957844351853793, 검증 손실: 0.3320732597350919\n","검증 손실이 감소하였습니다. (0.351305 에서 0.332073로). 모델 저장 중...\n","에폭 4, 훈련 손실: 0.30189008496093056, 검증 손실: 0.31305045721117736\n","검증 손실이 감소하였습니다. (0.332073 에서 0.313050로). 모델 저장 중...\n","에폭 5, 훈련 손실: 0.28080109473347464, 검증 손실: 0.3290671284179383\n","에폭 6, 훈련 손실: 0.26685399873717214, 검증 손실: 0.30405947294619184\n","검증 손실이 감소하였습니다. (0.313050 에서 0.304059로). 모델 저장 중...\n","에폭 7, 훈련 손실: 0.25266573339572274, 검증 손실: 0.28691561506220675\n","검증 손실이 감소하였습니다. (0.304059 에서 0.286916로). 모델 저장 중...\n","에폭 8, 훈련 손실: 0.24358083240986472, 검증 손실: 0.27730517923616027\n","검증 손실이 감소하였습니다. (0.286916 에서 0.277305로). 모델 저장 중...\n","에폭 9, 훈련 손실: 0.23140440839583457, 검증 손실: 0.28528395484460717\n","에폭 10, 훈련 손실: 0.22137868511880474, 검증 손실: 0.2988070339290021\n","에폭 11, 훈련 손실: 0.2145747832021552, 검증 손실: 0.28189355038711417\n","에폭 12, 훈련 손실: 0.20139728252857667, 검증 손실: 0.3046196813906339\n","에폭 13, 훈련 손실: 0.19774439470003818, 검증 손실: 0.3018855630917256\n","조기 종료 발동.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-ab787e0adf42>:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model.pth'))\n"]},{"output_type":"stream","name":"stdout","text":["10000개의 테스트 이미지에 대한 네트워크의 정확도: 89.77 %\n"]}]},{"cell_type":"markdown","source":["Task1_0829. 가상 데이터 생성 (generate_data 함수) 후 모델링 및 평가하세요\n","- generate_data 함수는 1000개의 랜덤 시퀀스 데이터를 생성합니다.\n","  - vocab_size: 시퀀스에 사용할 어휘의 크기를 설정합니다. 여기서는 100개의 단어를 사용합니다.\n","  - data: 각 시퀀스는 seq_length=10으로 설정된 10개의 정수(단어 인덱스)로 구성됩니다.\n","  - labels: 각 시퀀스에 대해 0 또는 1의 이진 레이블을 무작위로 할당합니다.\n","\n","- LSTM 기반 분류 모델을 정의하고, 가상 데이터로 학습 및 검증을 수행합니다.\n","- 조기 종료를 통해 학습 중 성능이 더 이상 개선되지 않을 때 학습을 중단합니다.\n","최종적으로 테스트 데이터로 최고 성능의 모델을 평가합니다."],"metadata":{"id":"R1ImHs5LPREA"}},{"cell_type":"markdown","source":["#### nn.Embedding(vocag_size,embed_dim)\n","- 텍스트 데이터(LSTM 모델): 텍스트 데이터는 이산적인 정수로 표현되므로, 이 정수들을 고차원 벡터로 매핑하는 nn.Embedding 계층이 필요합니다. 이 임베딩 계층은 단어 간의 의미적 유사성을 학습하는 데 유용합니다.\n","\n","- 이미지 데이터(CNN 모델): 이미지 데이터는 이미 공간적 구조를 가진 연속적인 값(픽셀)으로 표현되므로, 임베딩 계층이 필요하지 않습니다. 대신, 합성곱 계층이 이미지의 패턴을 학습하는 데 사용됩니다.\n","\n","LSTM 모델의 경우 텍스트 데이터의 정수 인덱스를 벡터로 변환하기 위해 nn.Embedding이 필요하지만, CNN 모델에서는 이미지 데이터를 처리하는 데 이미 직접적인 합성곱 연산이 사용되므로 임베딩 계층이 필요하지 않습니다.\n","\n","------------------------------------------------------------------------\n","\n","nn.BCEWithLogitsLoss는 PyTorch에서 이진 분류(Binary Classification) 작업을 수행할 때 주로 사용하는 손실 함수입니다. 이 함수는 이진 교차 엔트로피 손실(Binary Cross Entropy Loss)와 시그모이드(Sigmoid) 함수를 결합한 형태로 제공됩니다."],"metadata":{"id":"5FQOL0dQoJJ7"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","import numpy as np\n","\n","# 가상 데이터 생성 함수\n","def generate_data(num_samples=1000, seq_length=10):\n","    vocab_size = 100  # 단어 크기\n","    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n","    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n","    return data, labels\n","\n","# LSTM 분류 모델 정의\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        lstm_out, _ = self.lstm(x)\n","        out = self.fc(lstm_out[:, -1, :])\n","        return torch.sigmoid(out)\n","\n","# 훈련 및 검증 분할을 위한 데이터 인덱스 생성\n","data, labels = generate_data()\n","dataset = TensorDataset(data, labels)\n","\n","# 데이터 셋 분할\n","train_size = int(0.8 * len(data))\n","val_size = len(data) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# 데이터 로더 생성\n","trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","valloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# 테스트 데이터 생성\n","test_data, test_labels = generate_data(num_samples=10000)\n","test_dataset = TensorDataset(test_data, test_labels)\n","testloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# 모델, 손실함수, 옵티마이저 정의\n","model = LSTMClassifier(vocab_size=100, embedding_dim=100, hidden_dim=128, output_dim=1)\n","criterion = nn.BCELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# 조기 종료를 포함한 모델 학습\n","patience = 5\n","trials = 0\n","best_val_loss = np.Inf\n","\n","# 모델 반복 학습\n","for epoch in range(20):\n","    model.train()\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        labels = labels.view(-1, 1)\n","        labels = labels.float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    # 검증 단계\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for inputs, labels in valloader:\n","            labels = labels.view(-1, 1).float()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","    running_loss /= len(trainloader)\n","    val_loss /= len(valloader)\n","    print(f'에폭 {epoch + 1}, 훈련 손실: {running_loss:.6f}, 검증 손실: {val_loss:.6f}')\n","\n","    # 조기 종료 체크\n","    if val_loss < best_val_loss:\n","        print(f'검증 손실이 감소하였습니다. ({best_val_loss:.6f} 에서 {val_loss:.6f}로). 모델 저장 중...')\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model_task.pth')\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print('조기 종료 발동.')\n","            break\n","\n","# 모델 로드 후 테스트셋으로 평가\n","model.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model_task.pth'))\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        outputs = model(inputs)  # LSTM 모델에 입력 전달\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print(f'1000개 랜덤 시퀸스 대한 성능 :  {accuracy:.4f} %')"],"metadata":{"id":"F51j0mZMPU9y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724977404295,"user_tz":-540,"elapsed":4156,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"fd12e8ad-636f-413a-f190-1c276f436891"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["에폭 1, 훈련 손실: 0.694868, 검증 손실: 0.690755\n","검증 손실이 감소하였습니다. (inf 에서 0.690755로). 모델 저장 중...\n","에폭 2, 훈련 손실: 0.694448, 검증 손실: 0.690822\n","에폭 3, 훈련 손실: 0.693909, 검증 손실: 0.690940\n","에폭 4, 훈련 손실: 0.693466, 검증 손실: 0.691081\n","에폭 5, 훈련 손실: 0.693021, 검증 손실: 0.691213\n","에폭 6, 훈련 손실: 0.692614, 검증 손실: 0.691366\n","조기 종료 발동.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-b5a1cc56023b>:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('/content/drive/MyDrive/kdt_240424/m6_딥러닝/data/model/best_model_task.pth'))\n"]},{"output_type":"stream","name":"stdout","text":["1000개 랜덤 시퀸스 대한 성능 :  0.5070 %\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","\n","# 가상 데이터 생성 함수\n","def generate_data(num_samples=1000, seq_length=10):\n","    vocab_size = 100  # 어휘집 크기\n","    data = torch.randint(0, vocab_size, (num_samples, seq_length))\n","    labels = torch.randint(0, 2, (num_samples,))  # 0 또는 1의 레이블\n","    return data, labels\n","\n","data, labels = generate_data()\n","\n","# 텐서 데이터셋 및 데이터 로더 생성\n","dataset = TensorDataset(data, labels)\n","train_size = int(0.7 * len(dataset))\n","val_size = int(0.15 * len(dataset))\n","test_size = len(dataset) - (train_size + val_size)\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# LSTM 모델 클래스 정의\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        lstm_out, (hidden, _) = self.lstm(embedded)\n","        hidden = hidden[-1,:,:]\n","        return self.fc(hidden)\n","\n","model = LSTMModel(vocab_size=100, embed_dim=50, hidden_dim=100, output_dim=1)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters())\n","\n","# 훈련 함수\n","def train(model, train_loader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0\n","    for data, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(data).squeeze(1)\n","        loss = criterion(outputs, labels.float())\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","# 평가 함수\n","def evaluate(model, data_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    total_accuracy = 0\n","    with torch.no_grad():\n","        for data, labels in data_loader:\n","            outputs = model(data).squeeze(1)\n","            loss = criterion(outputs, labels.float())\n","            total_loss += loss.item()\n","            predictions = torch.round(torch.sigmoid(outputs))\n","            correct_predictions = (predictions == labels.unsqueeze(1)).float()\n","            total_accuracy += correct_predictions.sum().item()\n","    return total_loss / len(data_loader), total_accuracy / len(data_loader.dataset)\n","\n","# 조기 종료 로직을 포함한 훈련 및 검증 과정\n","best_val_loss = float('inf') # float('inf')는 파이썬에서 양의 무한대를 나타내는 방식\n","patience = 3\n","trials = 0\n","\n","for epoch in range(20):\n","    train_loss = train(model, train_loader, optimizer, criterion)\n","    val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        trials = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        trials += 1\n","        if trials >= patience:\n","            print(\"조기 종료 발생\")\n","            break\n","\n","# 테스트 데이터로 최고 모델 평가\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhySdcDSqpXQ","executionInfo":{"status":"ok","timestamp":1724977433930,"user_tz":-540,"elapsed":1235,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"7c5abcfe-b824-4ad6-e5c4-44c33df57d6b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Train Loss: 0.6937, Val Loss: 0.6914, Val Accuracy: 15.1467\n","Epoch 2, Train Loss: 0.6820, Val Loss: 0.6908, Val Accuracy: 15.3067\n","Epoch 3, Train Loss: 0.6699, Val Loss: 0.6928, Val Accuracy: 14.9333\n","Epoch 4, Train Loss: 0.6469, Val Loss: 0.6979, Val Accuracy: 15.0133\n","Epoch 5, Train Loss: 0.6101, Val Loss: 0.7174, Val Accuracy: 15.2000\n","조기 종료 발생\n","Test Loss: 0.7065, Test Accuracy: 14.1600\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-15-c109df005c34>:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model.pth'))\n"]}]}]}