{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3lok6rR3ErcXj4z41WI8x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YWePSq2kIYq_"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","# 분류모델 평가 함수\n","def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    y_proba = model.predict_proba(X_test)[:, 1]\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_proba)\n","    confusion = confusion_matrix(y_test, y_pred)\n","\n","    print(f'모델명: {model.__class__.__name__}')\n","    print(f'오차행렬:\\n{confusion:}')\n","    print(f'정확도: {accuracy:.4f}')\n","    print(f'정밀도:{precision:.4f}')\n","    print(f'재현율:{recall:.4f}')\n","    print(f'F1 score:{f1:.4f}')\n","    print(f'ROC-AUC:{roc_auc:.4f}')\n","    print('')"]},{"cell_type":"markdown","source":["1. 로지스틱 회귀 (Logistic Regression) : 독립 변수와 종속 변수 간의 관계를 시그모이드 함수로 나타내는 모델.\n"," - penalty: 사용되는 규제의 유형. 디폴트값: 'l2', 일반적인 설정: 'l1', 'l2', 'elasticnet'\n"," - C: 규제 강도를 조절하는 매개변수(역수). 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0\n"," - solver: 최적화 알고리즘. 디폴트값: 'lbfgs', 일반적인 설정: 'lbfgs', 'liblinear', 'sag', 'saga'\n"," - max_iter: 최적화 반복 횟수. 디폴트값: 100, 일반적인 설정: 100, 200, 1000\n","\n","2. 서포트 벡터 머신 (Support Vector Machine, SVM) : 최적의 결정 경계를 찾아 분류하는 모델, 커널 트릭 사용.\n"," - C: 규제 매개변수, 마진에 대한 오차 허용 범위. 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0\n"," - kernel: 커널 유형. 디폴트값: 'rbf', 일반적인 설정: 'linear', 'poly', 'rbf', 'sigmoid'\n"," - gamma: 커널 함수의 계수. 디폴트값: 'scale', 일반적인 설정: 'scale', 'auto'\n","\n","3. 결정 트리 (Decision Tree) : 데이터의 속성에 따라 구분되는 트리 구조 모델.\n"," - max_depth: 트리의 최대 깊이. 디폴트값: None, 일반적인 설정: 5, 10, None\n"," - min_samples_split: 노드를 분할하는 데 필요한 최소 샘플 수. 디폴트값: 2, 일반적인 설정: 2, 5, 10\n"," - min_samples_leaf: 리프 노드에 있어야 하는 최소 샘플 수. 디폴트값: 1, 일반적인 설정: 1, 2, 5\n"," - max_features: 각 분할에서 고려할 최대 특징 수. 디폴트값: None, 일반적인 설정: 'auto', 'sqrt', 'log2'\n","\n","4. 랜덤 포레스트 (Random Forest) : 여러 결정 트리를 결합한 앙상블 모델, 과적합 방지.\n"," - n_estimators: 트리의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - max_depth: 각 트리의 최대 깊이. 디폴트값: None, 일반적인 설정: 5, 10, None\n"," - min_samples_split: 노드를 분할하는 데 필요한 최소 샘플 수. 디폴트값: 2, 일반적인 설정: 2, 5, 10\n"," - min_samples_leaf: 리프 노드에 있어야 하는 최소 샘플 수. 디폴트값: 1, 일반적인 설정: 1, 2, 4\n"," - max_features: 각 트리에서 고려할 최대 특징 수. 디폴트값: 'auto', 일반적인 설정: 'auto', 'sqrt', 'log2'\n"," - bootstrap: 부트스트랩 샘플링 여부. 디폴트값: True, 일반적인 설정: True\n","\n","5. 그래디언트 부스팅 (Gradient Boosting) : 순차적인 약한 학습기의 앙상블로 강력한 학습기 생성.\n"," - n_estimators: 부스팅 단계의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - learning_rate: 각 단계의 기여도를 축소시키는 비율. 디폴트값: 0.1, 일반적인 설정: 0.01, 0.1, 0.2\n"," - max_depth: 각 트리의 최대 깊이. 디폴트값: 3, 일반적인 설정: 3, 5, 7\n"," - min_samples_split: 노드를 분할하는 데 필요한 최소 샘플 수. 디폴트값: 2, 일반적인 설정: 2, 5, 10\n"," - min_samples_leaf: 리프 노드에 있어야 하는 최소 샘플 수. 디폴트값: 1, 일반적인 설정: 1, 2, 4\n"," - subsample: 각 트리에서 사용되는 데이터의 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n","\n","6. 나이브 베이즈 (Naive Bayes) : 베이즈 정리를 기반으로 독립 가정을 사용하는 단순하고 빠른 모델.\n","- 가우시안 나이브 베이즈 (GaussianNB)\n"," - var_smoothing: 계산의 안정성을 위해 분산에 추가되는 값. 디폴트값: 1e-9, 일반적인 설정: 1e-9, 1e-8, 1e-7\n","- 멀티노미얼 나이브 베이즈 (MultinomialNB):\n"," - alpha: 라플라스(Laplacian) 스무딩 파라미터. 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0\n","\n","7. K-최근접 이웃 (K-Nearest Neighbors, KNN) : 입력 벡터와 가장 가까운 K개의 데이터 포인트의 클래스에 따라 분류.\n","\n"," - n_neighbors: 사용할 이웃의 수. 디폴트값: 5, 일반적인 설정: 3, 5, 10\n"," - weights: 가중치 함수. 디폴트값: 'uniform', 일반적인 설정: 'uniform', 'distance'\n"," - algorithm: 최근접 이웃 검색 알고리즘. 디폴트값: 'auto', 일반적인 설정: 'auto', 'ball_tree', 'kd_tree', 'brute'\n","\n","8. XGBoost : 고효율, 고성능의 그래디언트 부스팅 모델.\n"," - n_estimators: 부스팅 단계의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - learning_rate: 각 단계의 기여도. 디폴트값: 0.3, 일반적인 설정: 0.01, 0.1, 0.3\n"," - max_depth: 개별 트리의 최대 깊이. 디폴트값: 6, 일반적인 설정: 3, 5, 7\n"," - subsample: 각 트리에서 사용할 데이터 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - colsample_bytree: 각 트리에서 사용할 특징 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - gamma: 분할을 수행하기 위해 필요한 최소 손실 감소. 디폴트값: 0, 일반적인 설정: 0, 0.1, 0.3\n","\n","9. LightGBM 분류 (LGBMClassifier) : 대규모 데이터셋에서 빠르고 효율적인 그래디언트 부스팅 모델.\n"," - n_estimators: 부스팅 단계의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - learning_rate: 학습률. 각 단계의 기여도를 축소시키는 비율. 디폴트값: 0.1, 일반적인 설정: 0.01, 0.1, 0.3\n"," - max_depth: 트리의 최대 깊이를 제한하여 과적합을 방지. 디폴트값: -1 (제한 없음), 일반적인 설정: 3, 5, 7\n"," - num_leaves: 트리의 복잡성을 조절하는 주요 파라미터. 디폴트값: 31, 일반적인 설정: 31, 63, 127\n"," - min_data_in_leaf: 리프 노드의 최소 데이터 수를 설정하여 과적합을 방지. 디폴트값: 20, 일반적인 설정: 20, 50, 100\n"," - feature_fraction: 매 반복마다 선택할 특징의 비율을 결정, 과적합 방지. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - bagging_fraction: 부트스트랩 샘플링을 수행할 데이터 비율, 과적합 방지. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - bagging_freq: 배깅이 수행되는 빈도, 몇 단계마다 배깅을 수행할지 결정. 디폴트값: 0, 일반적인 설정: 1, 5\n"," - lambda_l1: 모델 복잡성을 제어하기 위한 L1 정규화의 강도. 디폴트값: 0.0, 일반적인 설정: 0.0, 0.1, 1.0\n"," - lambda_l2: 모델 복잡성을 제어하기 위한 L2 정규화의 강도. 디폴트값: 0.0, 일반적인 설정: 0.0, 0.1, 1.0\n"," - min_split_gain: 트리를 성장시키기 위한 분할을 허용하는 최소 손실 감소. 디폴트값: 0.0, 일반적인 설정: 0.0, 0.1, 0.5\n"," - boosting_type: 부스팅에 사용할 알고리즘 ('gbdt', 'dart', 'goss'). 디폴트값: 'gbdt', 일반적인 설정: 'gbdt'"],"metadata":{"id":"vPa9J8KoLQKo"}},{"cell_type":"code","source":["# if 멀티 분류시 평가 사용자 함수 정의.\n","def evaluate_model(model, X_test, y_test):\n","    # 예측 수행\n","    y_pred = model.predict(X_test)\n","\n","    # 분류 보고서 생성 : 멀티 클래스 분류시 ROC-AUC 계산\n","    report = classification_report(y_test, y_pred)\n","\n","    # ROC AUC 계산\n","    y_test_binarized = label_binarize(y_test, classes=['low', 'medium', 'high'])\n","    y_pred_prob = model.predict_proba(X_test)\n","    roc_auc = roc_auc_score(y_test_binarized, y_pred_prob, multi_class='ovr')\n","\n","    # 결과 출력\n","    print(f\"Test Accuracy: {accuracy:.4f}\")\n","    print(\"\\nClassification Report:\")\n","    print(report)\n","    print(f\"Test ROC AUC: {roc_auc:.4f}\")"],"metadata":{"id":"1Kmz-AFwXcz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 회귀모델 평가 함수\n","def get_model_cv_prediction(X_data, y_target):\n","    models = {\n","        'DecisionTreeRegressor': DecisionTreeRegressor(random_state=0, max_depth=4),\n","        'RandomForestRegressor': RandomForestRegressor(random_state=0, n_estimators=1000),\n","        'GradientBoostingRegressor': GradientBoostingRegressor(random_state=0, n_estimators=1000),\n","        'XGBRegressor': XGBRegressor(n_estimators=1000),\n","        'LGBMRegressor': LGBMRegressor(n_estimators=1000, verbose=-1)\n","    }\n","\n","    for model_name, model in models.items():\n","        neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\n","        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))\n","        print(f'{model_name} 5 폴드 세트의 평균 RMSE: {avg_rmse:.4f}')"],"metadata":{"id":"n-R1BVOzIuNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. 선형 회귀 (Linear Regression) : 독립 변수와 종속 변수 간의 선형 관계를 찾는 모델.\n"," - fit_intercept: 상수 항 포함 여부. 디폴트값: True, 일반적인 설정: True\n"," - normalize: 특성 정규화 여부. 디폴트값: False, 일반적인 설정: False\n","\n","2. 릿지 회귀 (Ridge Regression) : L2 정규화를 추가한 선형 회귀로, 과적합 방지.\n"," - alpha: L2 정규화 강도. 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0\n"," - solver: 최적화 알고리즘 선택. 디폴트값: 'auto', 일반적인 설정: 'auto', 'svd', 'lsqr', 'saga'\n","\n","3. 라쏘 회귀 (Lasso Regression) : L1 정규화를 추가한 선형 회귀로, 특징 선택 기능 제공. :\n"," - alpha: L1 정규화 강도. 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0, -\n"," - max_iter: 최대 반복 횟수. 디폴트값: 1000, 일반적인 설정: 1000, 5000, tol: 수렴 기준 허용 오차. 디폴트값: 0.0001, 일반적인 설정: 0.0001, 0.001\n","\n","4. 엘라스틱넷 회귀 (Elastic Net Regression) : L1과 L2 정규화를 결합한 모델로, 두 방식의 장점을 활용.\n"," - alpha: 정규화 강도. 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0\n"," - l1_ratio: L1과 L2 정규화 비율. 디폴트값: 0.5, 일반적인 설정: 0.1, 0.5, 0.9,\n"," - max_iter: 최대 반복 횟수. 디폴트값: 1000, 일반적인 설정: 1000, 5000, tol: 수렴 기준 허용 오차. 디폴트값: 0.0001, 일반적인 설정: 0.0001, 0.001\n","\n","5. 결정 트리 회귀 (Decision Tree Regression) : 데이터를 여러 구간으로 나누어 예측하는 트리 기반 모델.\n"," - max_depth: 트리의 최대 깊이. 디폴트값: None, 일반적인 설정: 5, 10, None, min_samples_split: 노드 분할을 위한 최소 샘플 수. 디폴트값: 2, 일반적인 설정: 2, 5, 10\n"," - min_samples_leaf: 리프 노드의 최소 샘플 수. 디폴트값: 1, 일반적인 설정: 1, 2, 5\n"," - max_features: 각 분할에서 고려할 최대 특징 수. 디폴트값: None, 일반적인 설정: 'auto', 'sqrt', 'log2'\n","\n","6. 랜덤 포레스트 회귀 (Random Forest Regression) : 여러 결정 트리를 앙상블하여 평균화하는 모델, 과적합 감소.\n"," - n_estimators: 트리의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - max_depth: 개별 트리의 최대 깊이. 디폴트값: None, 일반적인 설정: 5, 10, None\n"," - min_samples_split: 노드 분할을 위한 최소 샘플 수. 디폴트값: 2, 일반적인 설정: 2, 5, 10\n"," - min_samples_leaf: 리프 노드의 최소 샘플 수. 디폴트값: 1, 일반적인 설정: 1, 2, 4\n"," - max_features: 각 트리에서 고려할 최대 특징 수. 디폴트값: 'auto', 일반적인 설정: 'auto', 'sqrt', 'log2'\n"," - bootstrap: 부트스트랩 샘플링 여부. 디폴트값: True, 일반적인 설정: True\n","\n","7. 그래디언트 부스팅 회귀 (Gradient Boosting Regression) : 이전 모델의 오차를 보정하는 방식으로 트리들을 순차적으로 학습.\n"," - n_estimators: 부스팅 단계의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - learning_rate: 각 단계의 기여도. 디폴트값: 0.1, 일반적인 설정: 0.01, 0.1, 0.2\n"," - max_depth: 개별 트리의 최대 깊이. 디폴트값: 3, 일반적인 설정: 3, 5, 7\n"," - min_samples_split: 노드 분할을 위한 최소 샘플 수. 디폴트값: 2, 일반적인 설정: 2, 5, 10\n"," - min_samples_leaf: 리프 노드의 최소 샘플 수. 디폴트값: 1, 일반적인 설정: 1, 2, 4\n"," - subsample: 각 트리에서 사용되는 데이터 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n","\n","8. 서포트 벡터 회귀 (Support Vector Regression, SVR) : 결정 경계를 이용해 마진 내에서의 오차를 최소화하는 모델. :\n"," - C: 규제 매개변수, 오차 허용 범위. 디폴트값: 1.0, 일반적인 설정: 0.1, 1.0, 10.0\n"," - epsilon: 허용 오차 마진.디폴트값: 0.1, 일반적인 설정: 0.1, 0.2, 0.5\n"," - kernel: 커널 종류. 디폴트값: 'rbf', 일반적인 설정: 'linear', 'poly', 'rbf', 'sigmoid'\n"," - gamma: 커널 계수. 디폴트값: 'scale', 일반적인 설정: 'scale', 'auto'\n","\n","9. XGBoost 회귀 (XGBRegressor) : 효율적이고 높은 성능의 그래디언트 부스팅 모델.\n"," - n_estimators: 부스팅 단계의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - learning_rate (또는 eta): 각 단계의 기여도를 축소시키는 비율. 디폴트값: 0.3, 일반적인 설정: 0.01, 0.1, 0.3\n"," - max_depth: 개별 트리의 최대 깊이. 디폴트값: 6, 일반적인 설정: 3, 5, 7\n"," - subsample: 각 트리에서 사용할 데이터 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - colsample_bytree: 각 트리에서 사용할 특징 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - gamma: 분할을 수행하기 위해 필요한 최소 손실 감소. 디폴트값: 0, 일반적인 설정: 0, 0.1, 0.3\n"," - min_child_weight: 리프 노드가 되기 위한 최소 가중치 합. 디폴트값: 1, 일반적인 설정: 1, 3, 5\n"," - reg_alpha: L1 정규화 항의 가중치. 디폴트값: 0, 일반적인 설정: 0, 0.1, 1.0\n"," - reg_lambda: L2 정규화 항의 가중치. 디폴트값: 1, 일반적인 설정: 0.1, 1.0, 10.0\n","\n","10. LightGBM 회귀 (LGBMRegressor) : 속도와 메모리 효율이 높은 그래디언트 부스팅 모델.\n"," - n_estimators: 부스팅 단계의 수. 디폴트값: 100, 일반적인 설정: 100, 200, 500\n"," - learning_rate: 각 단계의 기여도를 축소시키는 비율. 디폴트값: 0.1, 일반적인 설정: 0.01, 0.1, 0.3\n"," - max_depth: 트리의 최대 깊이. 디폴트값: -1 (제한 없음), 일반적인 설정: 3, 5, 7\n"," - num_leaves: 하나의 트리가 가질 수 있는 최대 리프 노드 수. 디폴트값: 31, 일반적인 설정: 31, 63, 127\n"," - min_data_in_leaf: 리프 노드가 되기 위한 최소 데이터 수. 디폴트값: 20, 일반적인 설정: 20, 50, 100\n"," - feature_fraction: 각 트리에서 사용할 특징 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - bagging_fraction: 각 트리에서 사용할 데이터 샘플링 비율. 디폴트값: 1.0, 일반적인 설정: 0.8, 1.0\n"," - bagging_freq: 배깅이 수행되는 빈도 (0은 배깅을 수행하지 않음을 의미). 디폴트값: 0, 일반적인 설정: 1, 5\n"," - lambda_l1: L1 정규화 항의 가중치. 디폴트값: 0.0, 일반적인 설정: 0.0, 0.1, 1.0\n"," - lambda_l2: L2 정규화 항의 가중치. 디폴트값: 0.0, 일반적인 설정: 0.0, 0.1, 1.0\n","\n"," 11. SVM Regressor : 주어진 입력 데이터에 대해 예측값을 도출하며, 예측값과 실제 값 간의 오차를 최소화하는 것이 목표입니다.\n"," - C :모델의 복잡성과 오차 허용 범위 간의 균형을 조절합니다. 디폴트값: 1.0\n","일반적인 설정값: 일반적으로 0.1에서 1000까지의 값으로 설정하며, 문제에 따라 조정이 필요합니다.\n"," - epsilon : 회귀 모델이 허용하는 오차의 범위를 설정합니다. 디폴트값: 0.1. 일반적인 설정값: 0.01에서 1.0까지 다양하게 설정되며, 데이터의 특성에 따라 조정합니다.\n"," - gamma : RBF(가우시안) 커널과 같은 비선형 커널에서 데이터 포인트 간의 거리를 측정하는 데 사용. 디폴트값: scale (scikit-learn의 경우, 1 / (n_features * X.var())로 자동 설정됨) 일반적인 설정값: 0.001에서 1000까지 설정되며, 데이터의 스케일과 특성에 따라 조정합니다.\n"," - kernel : 데이터가 비선형일 때 커널 함수를 사용하여 입력 데이터를 고차원으로 변환. 디폴트값: rbf (Radial Basis Function 커널). 일반적인 설정값:\n","선형 커널 (linear): 간단한 선형 모델을 원하는 경우.\n","  - rbf : 비선형 관계를 모델링할 때 일반적으로 사용.\n","  - poly : 비선형 관계가 다항식 형태일 때 사용.\n"," - degree (다항식 커널의 경우) : 다항식 커널을 사용할 때, 다항식의 차수를 설정합니다. 높은 차수는 더 복잡한 모델을 생성할 수 있습니다. 디폴트값: 3. 일반적인 설정값: 2에서 5까지 설정되며, 모델의 복잡도와 데이터의 특성에 따라 조정합니다."],"metadata":{"id":"-GvJav30KF1X"}},{"cell_type":"code","source":["# 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 시각화한 함수 작성\n","def visualize_silhouette(cluster_lists, X_features):\n","\n","    from sklearn.datasets import make_blobs\n","    from sklearn.cluster import KMeans\n","    from sklearn.metrics import silhouette_samples, silhouette_score\n","\n","    import matplotlib.pyplot as plt\n","    import matplotlib.cm as cm\n","    import math\n","\n","    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함\n","    n_cols = len(cluster_lists)\n","\n","    # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성\n","    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)\n","\n","    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화\n","    for ind, n_cluster in enumerate(cluster_lists):\n","\n","        # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산.\n","        clusterer = KMeans(n_clusters = n_cluster, n_init='auto', max_iter=500, random_state=0)\n","        cluster_labels = clusterer.fit_predict(X_features)\n","\n","        sil_avg = silhouette_score(X_features, cluster_labels)\n","        sil_values = silhouette_samples(X_features, cluster_labels)\n","\n","        y_lower = 10\n","        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\\n' \\\n","                          'Silhouette Score :' + str(round(sil_avg,3)) )\n","        axs[ind].set_xlabel(\"The silhouette coefficient values\")\n","        axs[ind].set_ylabel(\"Cluster label\")\n","        axs[ind].set_xlim([-0.1, 1])\n","        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])\n","        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks\n","        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현.\n","        for i in range(n_cluster):\n","            ith_cluster_sil_values = sil_values[cluster_labels==i]\n","            ith_cluster_sil_values.sort()\n","\n","            size_cluster_i = ith_cluster_sil_values.shape[0]\n","            y_upper = y_lower + size_cluster_i\n","\n","            color = cm.nipy_spectral(float(i) / n_cluster)\n","            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \\\n","                                facecolor=color, edgecolor=color, alpha=0.7)\n","            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","            y_lower = y_upper + 10\n","\n","        axs[ind].axvline(x=sil_avg, color=\"red\", linestyle=\"--\")\n"],"metadata":{"id":"gbnRrNmCodeG"},"execution_count":null,"outputs":[]}]}