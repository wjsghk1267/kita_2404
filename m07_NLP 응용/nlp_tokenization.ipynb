{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPcJwalEMaQMAwIeYdOoGTR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 순환신경망(RNN) vs 트랜스포머\n","\n","순환신경망(RNN)\n","- 순차적 처리: RNN은 텍스트를 순차적으로 처리합니다. 이는 각 시점(time step)에서의 출력이 이전 시점의 출력에 의존하며, 이러한 방식으로 시퀀스 내의 정보를 전달합니다.\n","- 기억 능력: RNN은 이전 정보를 '기억'하는 능력을 가지고 있어, 시퀀스 내에서 정보를 전달할 수 있습니다. 그러나 장기 의존성 문제(long-term dependencies)로 인해 매우 긴 시퀀스에서 성능이 저하될 수 있습니다.\n","- 연산 병렬화 어려움: RNN의 순차적인 처리 특성으로 인해, 연산을 병렬로 수행하기 어렵습니다. 이는 학습 시간이 길어질 수 있다는 단점을 가집니다.\n","\n","트랜스포머\n","- 병렬 처리: 트랜스포머는 어탠션 메커니즘(Attention mechanism)을 사용하여 전체 시퀀스를 한 번에 처리합니다. 이를 통해 시퀀스 내의 모든 단어 간의 관계를 동시에 고려할 수 있으며, 연산을 효율적으로 병렬 처리할 수 있습니다.\n","- 어탠션 메커니즘: 트랜스포머의 핵심은 어탠션 메커니즘으로, 모델이 입력 시퀀스 내에서 중요한 정보에 '주의'를 집중하게 합니다. 이는 모델이 중요한 정보를 식별하고, 장기 의존성을 효과적으로 처리할 수 있게 돕습니다.\n","- 더 빠른 학습과 높은 성능: 병렬 처리 능력과 효율적인 장기 의존성 학습 덕분에 트랜스포머는 RNN에 비해 더 빠른 학습 속도와 더 높은 성능을 달성할 수 있습니다.\n","\n","RNN은 순차적인 데이터 처리에 강점을 가지고 있으나 장기 의존성 문제와 연산 병렬화의 어려움이 있습니다. 반면, 트랜스포머는 어탠션 메커니즘을 통해 전체 시퀀스를 한 번에 처리할 수 있어, 병렬 처리가 가능하고 장기 의존성을 효과적으로 다루며 더 빠른 학습과 높은 성능을 제공합니다."],"metadata":{"id":"ceyhVks8t8tF"}},{"cell_type":"markdown","source":["### Transformer\n","- Transformer 모델은 자연어 처리(NLP) 분야에서 주목받는 딥러닝 아키텍처. 2017년 \"Attention Is All You Need\" 논문을 통해 처음 소개되었다.\n","- 이 모델은 기존의 순환 신경망(RNN)이나 합성곱 신경망(CNN)에 비해 병렬 처리가 용이하고, 장거리 의존성을 효과적으로 학습할 수 있는 것이 특징.\n","- Transformer의 핵심 구성 요소는 Self-Attention 메커니즘과 Positional Encoding으로, 이를 통해 시퀀스 데이터를 더 효율적으로 처리할 수 있다.\n","\n","[ Transformer의 구조 ]\n","- Transformer 모델은 크게 인코더(encoder)와 디코더(decoder)의 두 부분으로 구성. 각 부분은 여러 개의 동일한 레이어로 구성되며, 각 레이어는 Multi-Head Attention과 Feed-Forward Neural Network로 이루어져 있다.\n","- 인코더는 입력 시퀀스를 처리하여 연속적인 표현(vector representation)으로 변환.\n","- 각 인코더 레이어는 Multi-Head Self-Attention 메커니즘과 포지션-와이즈(Position-wise) 피드포워드 신경망으로 구성.\n","- Self-Attention 메커니즘은 입력 시퀀스의 각 단어가 다른 모든 단어와의 관계를 평가하여, 문맥에 따른 단어의 중요도를 파악.\n","- 디코더는 인코더로부터의 출력과 함께 이전에 생성된 출력 시퀀스를 받아 새로운 출력(예: 번역된 문장)을 생성.\n","- 디코더 레이어는 인코더 레이어와 유사하지만, 추가적으로 인코더의 출력을 활용하는 Encoder-Decoder Attention 레이어를 포함한다.\n","- 이 Attention 레이어는 디코더가 인코더의 출력에 주목함으로써 입력 시퀀스와 출력 시퀀스 사이의 관계를 학습한다.\n","\n","[ 핵심 기술 ]\n","- Self-Attention은 시퀀스 내의 각 요소가 시퀀스 내의 다른 모든 요소와의 관계를 계산합니다. 이를 통해 모델은 각 단어의 문맥적 의미를 파악할 수 있다.\n","- Multi-Head Attention은 여러 개의 Attention 메커니즘을 병렬로 사용하여, 다양한 표현(subspace)에서 정보를 동시에 학습.. 이로써 모델은 더 복잡한 문맥 관계를 이해할 수 있다.\n","- Transformer는 순서 정보가 없는 Self-Attention 메커니즘을 사용하기 때문에, 입력 시퀀스의 단어 위치 정보를 모델에 주입하기 위해 Positional Encoding을 사용한다. 이는 각 단어의 절대적 혹은 상대적 위치 정보를 담은 벡터를 단어의 표현에 추가함으로써 구현된다.\n","- Transformer 모델은 이러한 구조와 기술을 통해 시퀀스를 효과적으로 처리하며, 이는 기계 번역, 문서 요약, 질문 응답 시스템 등 다양한 NLP 태스크에서 뛰어난 성능을 발휘. Transformer의 성공 이후, 이를 기반으로 한 다양한 변형 모델들(BERT, GPT 등)이 개발되어 NLP 분야의 발전을 이끌고 있다."],"metadata":{"id":"S2rl6oLcz-bz"}},{"cell_type":"markdown","source":["### BiDirectional Encoder Representations from Transformers\n","- BERT는 Google이 개발한 자연어 처리(NLP)에 대한 혁신적인 접근 방식이다. Jacob Devlin과 Google의 동료들이 2018년 논문에서 소개한 BERT는 컴퓨터가 전례 없는 정확도로 인간의 언어를 이해할 수 있게 함으로써 NLP 분야에 큰 영향을 주었다.\n","\n","1. 구조\n","- BERT는 입력 데이터를 처리하기 위해 주의 메커니즘을 사용하는 Transformer 아키텍처를 기반으로 한다.\n","- 텍스트 입력을 순차적으로(왼쪽에서 오른쪽 또는 오른쪽에서 왼쪽) 읽는 기존 모델과 달리 BERT는 텍스트를 양방향으로 처리. 즉, 단어 앞과 뒤의 단어를 살펴봄으로써 단어의 문맥을 검토하여 텍스트를 더욱 깊이 이해하게 된다.\n","\n","2. 사전 훈련 및 미세 조정\n","- BERT의 개발에는 사전 훈련과 미세 조정이라는 두 단계가 포함되며 사전 학습 중에 모델은 MLM(마스크 언어 모델링)과 NSP(다음 문장 예측)라는 두 가지 비지도 작업을 통해 대규모 텍스트 데이터 모음(예: Wikipedia 및 BooksCorpus)에 대해 학습된다. MLM에는 입력 토큰의 일정 비율을 무작위로 숨긴 다음 마스크된 토큰을 예측하는 작업이 포함되며 NSP에는 두 개의 텍스트 세그먼트가 원본 텍스트에 연속적으로 나타나는지 예측하는 작업이 포함된다.\n","- 사전 훈련 후 BERT는 단 하나의 추가 출력 레이어로 미세 조정되어 실질적인 수정 없이 질문 답변, 감정 분석, 언어 추론과 같은 광범위한 NLP 작업을 위한 최첨단 모델을 생성할 수 있다.\n","3. 영향\n","- BERT의 양방향 훈련과 상대적으로 작은 데이터 세트로 특정 작업을 미세 조정할 수 있는 능력은 광범위한 NLP 애플리케이션에 매우 효과적이다. BERT 도입으로 NLP 작업에 접근하는 방식이 크게 바뀌었고 수많은 벤치마크와 경쟁에서 기록적인 성능을 달성했다.\n","\n"],"metadata":{"id":"grjPGR_u3vn5"}},{"cell_type":"markdown","source":["### GPT(Generative Pre-trained Transformer)\n","- OpenAI가 개발한 자연어 처리(NLP) 작업을 위해 설계된 인공 지능 모델의 일종.\n","- GPT는 변환기 기반 모델 제품군에 속하며, 입력 데이터 내의 다양한 단어의 중요성을 평가하는 주의 메커니즘을 사용하여 텍스트와 같은 데이터 시퀀스를 처리하는 기능으로 알려져 있다.\n","\n","1. 구조\n","- GPT는 특히 self-attention 메커니즘으로 유명한 Transformer 아키텍처를 활용하며 이를 통해 GPT는 각 단어를 개별적으로 또는 고정된 순서로 처리하는 대신 전체 단어 순서를 고려하여 문장에 있는 각 단어의 맥락을 효과적으로 이해할 수 있다.\n","\n","2. 사전 훈련\n","- GPT는 감독되지 않은 방식으로 대규모 텍스트 데이터 모음에 대해 사전 학습되었다. 사전 훈련에는 다음 단어 예측 작업을 활용하여 이전 단어가 주어지면 문장에서 다음 단어를 예측하는 방법을 학습하는 것이 포함됩니다. 이러한 광범위한 사전 학습을 통해 GPT는 언어 패턴, 문법, 세상에 대한 지식을 폭넓게 이해할 수 있다.\n","\n","3. 미세 조정\n","- 사전 학습 후 텍스트 생성, 번역, 질문 답변 등과 같은 특정 NLP 작업에 맞게 GPT를 미세 조정할 수 있다. 미세 조정에는 더 작은 작업별 데이터 세트에 대한 추가 교육이 포함되므로 GPT는 특정 작업을 잘 수행하기 위해 일반화된 언어 이해를 조정할 수 있다.\n","\n","4. 생성 기능\n","- GPT의 특징 중 하나는 일관되고 상황에 맞는 텍스트를 생성하는 능력이다. 인간이 작성한 텍스트와 종종 구별할 수 없는 방식으로 문장, 단락 또는 전체 기사를 완성할 수 있다. 이러한 생성 기능으로 인해 GPT는 콘텐츠 제작, 대화 시스템 등을 포함한 광범위한 애플리케이션에 매우 효과적이다.\n","\n","5. 버전\n","- GPT-1부터 시작하여 GPT-2, GPT-3, GPT-3.5, GPT-4 등과 같은 주목할만한 버전을 포함하여 각각 이전 버전보다 더 강력하고 복잡한 여러 버전의 GPT가 있다. 각각의 새 버전에는 일반적으로 더 많은 수의 매개변수(예: 훈련 데이터에서 학습된 모델 요소)가 포함되어 있어 보다 미묘한 이해와 텍스트 생성이 가능하다.\n","- GPT 모델은 NLP 분야에 큰 영향을 미쳤으며 텍스트 생성 및 이해 분야에서 놀라운 능력을 보여주고 AI가 언어 작업에서 달성할 수 있는 것에 대한 새로운 표준을 설정했다."],"metadata":{"id":"PZ8GbbxuJKZc"}},{"cell_type":"markdown","source":["## 트랜스퍼 러닝\n","- 트랜스퍼 러닝(transfer learning)이란 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법\n","- 트랜스퍼 러닝이 주목받게 된 것은 업스트림 태스크와 프리트레인 덕분. 자연어의 풍부한 문맥(context)을 모델에 내재화하고 이 모델을 다양한 다운스트림 태스크에 적용해 그 성능을 대폭 끌어올림\n","- 대표적인 업스트림 태스크 가운데 하나가 다음 단어 맞추기(언어모델)입니다. GPT 계열 모델이 바로 이 태스크로 프리트레인을 수행\n","- 다른 업스트림 태스크로는 빈칸 채우기가 있습니다. BERT 계열 모델이 바로 이 태스크로 프리트레인을 수행. ‘빈칸 채우기’로 업스트림 태스크를 수행한 모델을 마스크 언어 모델(Masked Language Model)\n","- 다음 단어 맞히기, 빈칸 채우기 같은 업스트림 태스크는 강력한 힘을 지니며 뉴스, 웹 문서, 백과사전 등 글만 있으면 수작업 없이도 다량의 학습 데이터를 아주 싼값에 만들어 낼 수 있다. 이처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 자기 지도 학습(self-supervised learning)이라고 함.\n","- 다운스트림 태스크의 학습 방식은 모두 파인튜닝(fine-tuning)이며 프리트레인을 마친 모델을 다운스트림 태스크에 맞게 업데이트하는 기법. 다운스트림 태스크는 자연어 처리의 구체적인 과제들이다."],"metadata":{"id":"IJMYZ11NJKVo"}},{"cell_type":"markdown","source":["## Transformer 모델 vs 허깅페이스(Hugging Face)의 Transformers 라이브러리\n","\n","### 1. **Transformer 모델**\n","- **정의**: Transformer는 2017년에 Vaswani et al.이 제안한 딥러닝 모델 아키텍처입니다. 이 모델은 자연어 처리(NLP) 작업에서 뛰어난 성능을 보여주며, 순차적인 입력 데이터(예: 텍스트)를 병렬 처리하는 데 매우 효과적입니다.\n","- **구성 요소**:\n","  - **Self-Attention Mechanism**: 입력의 각 부분이 다른 모든 부분과의 관계를 평가할 수 있게 하여, 문맥을 잘 이해하게 합니다.\n","  - **Encoder-Decoder 구조**: 일반적으로 번역 작업 등에서 사용되며, 입력 시퀀스를 인코딩하고, 이를 기반으로 출력 시퀀스를 생성합니다.\n","  - **Variations**: BERT, GPT, T5 등 다양한 파생 모델들이 Transformer 아키텍처를 기반으로 개발되었습니다.\n","\n","### 2. **Hugging Face의 Transformers 라이브러리**\n","- **정의**: Hugging Face의 Transformers 라이브러리는 다양한 사전 학습된 Transformer 모델(BERT, GPT, T5 등)을 쉽게 사용할 수 있도록 해주는 파이썬 라이브러리입니다. 이 라이브러리는 NLP 작업을 위한 사전 학습된 모델을 손쉽게 불러오고, 파인튜닝(finetuning)하거나 새로운 작업에 적용할 수 있게 도와줍니다.\n","- **주요 기능**:\n","  - **사전 학습된 모델 제공**: BERT, GPT, T5 등 여러 사전 학습된 모델을 제공하며, 이를 손쉽게 로드하여 사용할 수 있습니다.\n","  - **파인튜닝 및 추론 지원**: 특정 NLP 작업(예: 문장 분류, 질의응답, 번역 등)에 대해 모델을 파인튜닝하고, 추론 작업을 수행할 수 있습니다.\n","  - **커뮤니티 중심**: 사용자가 자신이 훈련한 모델을 공유하거나, 다른 사용자가 제공한 모델을 사용할 수 있는 허브 기능을 제공합니다.\n","\n","### 요약\n","- **Transformer 모델**: 특정한 아키텍처를 지칭하는 용어로, 자연어 처리와 같은 순차적 데이터 작업에서 널리 사용되는 신경망 구조입니다.\n","- **Hugging Face의 Transformers 라이브러리**: 다양한 Transformer 기반 모델을 쉽게 사용할 수 있게 해주는 도구 모음으로, 사전 학습된 모델의 로드, 파인튜닝, 추론 작업을 단순화합니다.\n","\n","따라서, Transformer 모델은 기본 아키텍처를 말하는 것이고, Hugging Face의 Transformers 라이브러리는 이 모델들을 실용적으로 활용하기 위한 라이브러리라고 할 수 있습니다."],"metadata":{"id":"ZkH_hqAEJKRQ"}},{"cell_type":"markdown","source":["토크나이저(tokenizer)는 자연어 처리(NLP)에서 텍스트 데이터를 모델이 이해할 수 있는 형식으로 변환하는 중요한 도구입니다. 구체적으로, 텍스트를 의미 단위로 분리하는 과정을 담당하는데, 이 의미 단위가 토큰(token)입니다. 토크나이저는 주어진 문장을 단어, 부분 단어, 문자 또는 서브워드 단위로 분리하여 각 토큰을 정수로 변환합니다. 이렇게 변환된 정수는 모델의 입력으로 사용될 수 있습니다.\n","\n","### 1. **토크나이저의 필요성**\n","자연어는 모델이 바로 이해할 수 있는 형식이 아니므로, 텍스트 데이터를 수치 데이터로 변환하는 과정이 필요합니다. 토크나이저는 이 변환 과정을 담당하는 중요한 역할을 합니다. 예를 들어, 문장을 단어 또는 서브워드 단위로 분리하고, 이를 고유한 인덱스(정수)로 변환함으로써 모델이 해당 데이터를 학습할 수 있도록 돕습니다.\n","\n","### 2. **토크나이저의 종류**\n","토크나이저는 텍스트 데이터를 분리하는 방식에 따라 다양한 유형이 있습니다.\n","\n","#### (1) **단어 수준 토크나이저 (Word-level Tokenizer)**\n","- 문장을 단어 단위로 나누는 토크나이저입니다.\n","- 예: 문장 \"I love NLP.\" → [\"I\", \"love\", \"NLP\", \".\"]\n","- 단점: 사전에 없는 희귀 단어나 새로운 단어(예: 고유명사)에 대해 제대로 처리하지 못할 수 있습니다.\n","\n","#### (2) **문자 수준 토크나이저 (Character-level Tokenizer)**\n","- 문장을 문자 단위로 나누는 방식입니다.\n","- 예: 문장 \"I love NLP.\" → [\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"N\", \"L\", \"P\", \".\"]\n","- 매우 세분화된 표현이 가능하지만, 길이가 매우 긴 시퀀스를 생성하게 되어 처리 효율이 낮을 수 있습니다.\n","\n","#### (3) **서브워드 수준 토크나이저 (Subword-level Tokenizer)**\n","- 단어와 문자의 중간 수준으로, 단어를 서브워드(부분 단어) 단위로 나눕니다. 최신 Transformer 모델들이 주로 사용하는 방식입니다.\n","- 주로 **BPE(Byte Pair Encoding)**나 **WordPiece** 등의 알고리즘을 사용하여 희귀한 단어를 처리하면서도 효율적인 표현을 유지합니다.\n","- 예: 문장 \"unbelievable\" → [\"un\", \"##believable\"] (WordPiece)\n","- 새로운 단어를 처리할 때 유용하며, 모델의 학습 효율성을 높입니다.\n","\n","### 3. **서브워드 기반 토크나이저의 예**\n","- **BPE (Byte Pair Encoding)**: 자주 등장하는 문자 쌍을 병합하면서 서브워드를 생성하는 방식입니다. 단어를 문자 쌍으로 분해한 뒤, 빈도가 높은 문자 쌍을 하나의 서브워드로 병합해 나가면서 단어를 표현합니다.\n","  \n","- **WordPiece**: BPE와 유사하게 자주 등장하는 서브워드로 단어를 구성하지만, 조금 더 정교한 확률 기반 방법을 사용합니다. BERT 모델에서 사용된 방식입니다.\n","\n","- **SentencePiece**: Google이 개발한 토크나이저로, BPE 및 Unigram 알고리즘을 기반으로 하며, 문장을 미리 정의된 서브워드 또는 단일 문자로 분리합니다. 주로 언어 모델 학습 시 널리 사용됩니다.\n","\n","### 4. **토크나이저의 출력**\n","토크나이저는 단순히 텍스트를 나누는 것에 그치지 않고, 그 결과를 숫자형 벡터로 변환하여 모델이 처리할 수 있는 형식으로 제공합니다. 이 과정을 토큰화라 하며, 주로 다음의 출력 요소를 포함합니다:\n","\n","- **토큰 ID**: 각 토큰에 해당하는 고유한 숫자. 예: [\"I\", \"love\", \"NLP\"] → [1, 15, 99] (각 단어에 고유 ID 할당)\n","- **어텐션 마스크**: 패딩된 토큰과 실제 유효한 토큰을 구별하기 위한 마스크 벡터. 0은 패딩, 1은 실제 토큰을 나타냅니다.\n","- **세그먼트 ID**: BERT와 같은 모델에서 문장의 구분을 위해 사용되며, 첫 번째 문장은 0, 두 번째 문장은 1로 표시됩니다.\n","\n","### 5. **Hugging Face의 토크나이저**\n","Hugging Face는 자연어 처리 모델을 위한 다양한 토크나이저를 제공합니다. `transformers` 라이브러리에서 지원하는 토크나이저는 BERT, GPT, T5 등 다양한 모델과 함께 사용할 수 있으며, 자동으로 각 모델에 맞는 토크나이징을 수행합니다.\n","\n","```python\n","from transformers import BertTokenizer\n","\n","# BERT 토크나이저 불러오기\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 텍스트 토큰화\n","tokens = tokenizer.tokenize(\"I love NLP.\")\n","print(tokens)  # 출력: ['i', 'love', 'nlp', '.']\n","\n","# 토큰을 ID로 변환\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(token_ids)  # 출력: [1045, 2293, 10938, 1012]\n","```\n","\n","### 6. **토크나이저의 중요성**\n","토크나이저는 단순한 전처리 도구가 아닌, NLP 모델의 성능에 큰 영향을 미치는 중요한 요소입니다. 특히 서브워드 수준의 토크나이저는 어휘 크기를 줄이면서도 새로운 단어를 유연하게 처리할 수 있어, 모델이 다양한 언어 및 도메인에서 더 잘 일반화될 수 있도록 돕습니다."],"metadata":{"id":"KKKaCOHMJQr_"}},{"cell_type":"markdown","source":["Byte Pair Encoding(BPE)는 서브워드 기반의 토크나이저 중 하나로, 텍스트를 효율적으로 처리하기 위해 자주 사용되는 기법입니다. BPE는 문장 또는 단어를 자주 등장하는 문자 쌍으로 결합해 새로운 서브워드를 생성하는 방식으로 동작합니다. 이 방식을 통해 어휘 사전 크기를 줄이면서도 새로운 단어를 유연하게 처리할 수 있습니다.\n","\n","### BPE 토큰화 과정 예시:\n","```\n","hello, helium, help, shell\n","```\n","\n","### 1. **초기 설정: 모든 단어를 문자로 나누기**\n","먼저 모든 단어를 문자 단위로 나누고, 각 단어 끝에 `</w>`를 추가합니다.\n","\n","```\n","h e l l o </w>\n","h e l i u m </w>\n","h e l p </w>\n","s h e l l </w>\n","```\n","\n","### 2. **문자 쌍 빈도 계산**\n","각 문자 쌍의 빈도를 계산합니다:\n","\n","- `(\"h\", \"e\")`: 3번\n","- `(\"e\", \"l\")`: 4번\n","- `(\"l\", \"l\")`: 2번\n","- `(\"l\", \"i\")`: 1번\n","- `(\"l\", \"p\")`: 1번\n","- `(\"s\", \"h\")`: 1번\n","\n","가장 빈도가 높은 쌍은 **`(\"e\", \"l\")`**입니다.\n","\n","### 3. **가장 자주 등장하는 문자 쌍 병합**\n","`(\"e\", \"l\")`을 병합합니다. 병합 결과는 다음과 같습니다:\n","\n","```\n","h el l o </w>\n","h el i u m </w>\n","h el p </w>\n","s h el l </w>\n","```\n","\n","### 4. **다시 빈도 계산 후 병합 반복**\n","이제 다시 문자 쌍의 빈도를 계산합니다:\n","\n","- `(\"h\", \"el\")`: 3번\n","- `(\"el\", \"l\")`: 2번\n","- `(\"l\", \"o\")`: 1번\n","- `(\"l\", \"i\")`: 1번\n","- `(\"l\", \"p\")`: 1번\n","- `(\"s\", \"h\")`: 1번\n","\n","가장 자주 등장하는 문자 쌍은 **`(\"h\", \"el\")`**입니다.\n","\n","### 5. **`(\"h\", \"el\")` 병합**\n","`(\"h\", \"el\")`을 병합합니다. 병합 결과는 다음과 같습니다:\n","\n","```\n","hel l o </w>\n","hel i u m </w>\n","hel p </w>\n","s hel l </w>\n","```\n","\n","### 6. **다시 빈도 계산**\n","다시 문자 쌍의 빈도를 계산합니다:\n","\n","- `(\"hel\", \"l\")`: 2번\n","- `(\"l\", \"o\")`: 1번\n","- `(\"l\", \"i\")`: 1번\n","- `(\"l\", \"p\")`: 1번\n","- `(\"s\", \"h\")`: 1번\n","\n","가장 자주 등장하는 문자 쌍은 **`(\"hel\", \"l\")`**입니다.\n","\n","### 7. **`(\"hel\", \"l\")` 병합**\n","`(\"hel\", \"l\")`을 병합합니다. 병합 결과는 다음과 같습니다:\n","\n","```\n","hell o </w>\n","hel i u m </w>\n","hel p </w>\n","s hell </w>\n","```\n","\n","### 8. **다시 빈도 계산**\n","이제 다시 빈도를 계산합니다. 빈도가 모두 1이므로 모든 쌍이 동일한 빈도를 가집니다:\n","\n","- `(\"hell\", \"o\")`: 1번\n","- `(\"hel\", \"i\")`: 1번\n","- `(\"hel\", \"p\")`: 1번\n","- `(\"s\", \"hell\")`: 1번\n","\n","**이 경우에는 텍스트에서 순서상 먼저 나타나는 문자 쌍을 병합**합니다. 즉, `\"hell o </w>\"`에서 `\"hell\"`과 `\"o\"` 쌍이 먼저 등장하므로, 이 쌍을 병합합니다.\n","\n","### 9. **`(\"hell\", \"o\")` 병합 과정**\n","\n","이 단계에서, 우리가 가지고 있는 데이터는 아래와 같습니다:\n","\n","```\n","hell o </w>\n","hel i u m </w>\n","hel p </w>\n","s hell </w>\n","```\n","\n","이 상황에서, 모든 문자 쌍의 빈도는 1입니다. 이제 BPE는 문자 쌍의 빈도가 같다면 **텍스트에서 가장 먼저 등장하는 쌍**을 병합합니다. 여기서 가장 먼저 등장하는 쌍은 **`(\"hell\", \"o\")`**입니다. 따라서 이 쌍을 병합하게 됩니다.\n","\n","병합 결과는:\n","\n","```\n","hello </w>\n","hel i u m </w>\n","hel p </w>\n","s hell </w>\n","```\n","\n","여기서 \"hell\"과 \"o\"가 병합되어 하나의 토큰 **`\"hello\"`**가 되었고, 단어의 끝을 나타내는 `</w>`는 그대로 남아 있습니다. 이 단계에서는 여전히 `\"hello\"`와 `</w>`가 따로 존재하지만, 마지막으로 `</w>`를 포함한 병합이 이루어집니다.\n","\n","### 10. **\"hello \\</w>\"가 \"hello\\</w>\"로 병합되는 과정**\n","\n","이제 병합해야 할 쌍은 **`(\"hello\", \"</w>\")`**입니다. `</w>`는 BPE에서 단어의 끝을 나타내는 특수 기호로, 단어의 끝을 마무리하는 역할을 합니다.\n","\n","병합을 진행하면, `\"hello\"`와 `</w>`가 하나의 토큰으로 병합되어 **`\"hello</w>\"`**가 됩니다. 이제 **\"hello \\</w>\"**가 **\"hello\\</w>\"**라는 하나의 토큰으로 병합됩니다.\n","\n","최종 결과:\n","\n","```\n","hello</w>\n","hel i u m </w>\n","hel p </w>\n","s hell </w>\n","```\n","\n","### 병합 후 어휘 설명\n","\n","BPE는 반복적으로 문자 쌍을 병합하여 서브워드를 생성하고, 이 과정에서 `</w>`는 항상 마지막에 있는 토큰과 결합되어 단어의 끝을 나타냅니다. 최종적으로 어휘는 아래와 같이 만들어집니다:\n","\n","```\n","[\"hello</w>\", \"hel\", \"i\", \"u\", \"m</w>\", \"p</w>\", \"s\", \"hell</w>\"]\n","```\n","\n","이 어휘에서:\n","- `\"hello</w>\"`는 완성된 단어 `\"hello\"`를 나타냅니다.\n","- `\"hel\"`, `\"i\"`, `\"u\"`, `\"m</w>\"` 등은 각각 서브워드로, `</w>`가 붙어 있는 것은 단어 끝을 나타냅니다.\n","\n","### 최종 요약:\n","- **`</w>`**는 단어의 끝을 나타내는 기호로, BPE 병합 과정에서 마지막으로 단어와 결합됩니다.\n","- 예를 들어, `\"hello\"`와 `</w>`가 병합되어 **`\"hello</w>\"`**라는 하나의 토큰으로 변환됩니다.\n","- 이 과정을 통해 단어와 끝 기호가 하나의 토큰으로 완성되어, 모델이 단어의 끝을 명확히 구분할 수 있게 됩니다.\n","\n"],"metadata":{"id":"ZzXK1Kd8JTO0"}},{"cell_type":"markdown","source":["## 워드피스(wordpiece)\n","- 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사. 다만 어휘 집합을 구축할 때 문자열을 병합하는 기준이 다르다. 워드피스는 BPE처럼 단순히 빈도를 기준으로 병합하는 것이 아니라, 병합했을 때 말뭉치의 우도(likelihood)를 가장 높이는 쌍을 병합.\n","- 말뭉치의 우도 증가\"에 관해 이야기할 때, 이는 모델의 언어 표현이 실제 언어에서 발견되는 패턴과 시퀀스를 보다 정확하게 반영하도록 모델 또는 해당 매개변수를 조정하는 것을 의미\n","- 기술 뉴스 기사 영역에 초점을 맞춰 텍스트를 토큰화하고 이해하기 위한 모델에 대한 어휘를 구축하고 있다고 가정하면 코퍼스는 수많은 기술 뉴스 기사로 구성되어 있으며 처음에 텍스트에는 \"스마트폰\", \"애플리케이션\" 및 \"네트워크\"라는 용어가 자주 등장한다. 모델은 \"스마트폰\"이라는 단어를 초기 어휘에서 별도의 토큰인 \"스마트\" + \"폰\"으로 토큰화할 수 있는 이것은 틀린 것은 아니지만 특히 기술 기사에서 \"스마트폰\"의 빈도와 구체적인 의미를 고려하면 모델은 더 효율적일 수 있는 \"스마트폰\"을 단일 토큰으로 추가하는 것을 평가한다.스마트폰\"을 어휘에 통합함으로써 모델은 이제 기술 뉴스 기사의 말뭉치를 보다 정확하고 간결하게 표현할 수 있기 때문이다. 어휘에 단일 토큰으로 \"스마트폰\"이 존재하면 해당 어휘가 관찰된 데이터(기술 뉴스 기사)를 생성하거나 나타낼 수 있을 우도(가능성)이 더 높아진다. 정확하게. 이 업데이트된 어휘를 고려할 때 말뭉치의 우도는 단일 토큰인 \"스마트폰\"이 별도의 토큰인 \"스마트\" + \"전화\"보다 더 효율적으로 개념을 포착할 수 있기 때문에 증가한다."],"metadata":{"id":"AjAu4wpzJUVc"}},{"cell_type":"markdown","source":["#### Hugging Face의 **`tokenizers`** 라이브러리\n","자연어 처리(NLP)에서 텍스트를 효율적으로 처리하기 위한 **토크나이저**(tokenizer)들을 제공하는 고성능 툴킷입니다. 이 라이브러리는 특히 매우 빠르고 효율적인 토크나이저를 제공하며, 다양한 토큰화 전략을 지원합니다. **Transformer** 기반의 NLP 모델을 위한 텍스트 전처리에 최적화되어 있어, BERT, GPT, T5 등 최신 NLP 모델들과 함께 사용할 수 있습니다.\n","\n","### 주요 특징\n","\n","1. **고성능과 속도**:\n","   - `tokenizers` 라이브러리는 Rust 언어로 작성되어 있기 때문에 기존의 Python 기반 토크나이저보다 **훨씬 빠릅니다**. 수백 MB에서 수 GB에 이르는 대규모 텍스트 데이터를 매우 짧은 시간에 처리할 수 있습니다.\n","   - 병렬 처리가 가능하여, 다중 스레드 환경에서 성능을 더욱 극대화할 수 있습니다.\n","\n","2. **메모리 효율성**:\n","   - 이 라이브러리는 메모리 사용량이 적고, 대규모 데이터를 처리할 때도 효율적으로 동작합니다. 대용량 데이터를 처리해야 하는 NLP 작업에서 매우 유용합니다.\n","\n","3. **여러 가지 토큰화 방법 지원**:\n","   - Hugging Face의 `tokenizers`는 다양한 토크나이징 기법을 지원합니다. 가장 대표적인 토크나이징 방법으로는 다음이 있습니다:\n","     - **BPE (Byte-Pair Encoding)**: 자주 사용되는 서브워드를 병합해 어휘를 구성하는 방식.\n","     - **WordPiece**: BERT에서 사용된 방식으로, BPE와 유사하지만 다른 병합 규칙을 사용합니다.\n","     - **Unigram Language Model**: SentencePiece에서 사용하는 방식으로, 최적의 서브워드를 선택하는 방식입니다.\n","     - **Byte-level BPE**: 텍스트를 바이트 단위로 처리하여, 아스키 코드 이외의 문자들도 자연스럽게 처리할 수 있는 방법입니다.\n","     - **Whitespace Tokenizer**: 공백 단위로 단순히 토큰을 구분하는 방법입니다.\n","   - 이 다양한 기법들은 각각의 특성에 따라 텍스트를 최적으로 처리할 수 있습니다.\n","\n","4. **Pre-tokenization**:\n","   - 토크나이저는 본격적인 토큰화에 앞서 **Pre-tokenization** 과정을 수행하는데, 이는 텍스트를 토큰화하기 전에 공백, 구두점 등을 처리하는 단계입니다. 이를 통해 모델이 처리해야 할 텍스트의 품질을 높일 수 있습니다.\n","\n","5. **정교한 토큰 처리**:\n","   - **특수 토큰 관리**: `[PAD]`, `[CLS]`, `[SEP]` 등 모델이 학습할 때 필요한 특수 토큰들을 추가하고 관리하는 기능을 제공합니다.\n","   - **어휘 사전 및 병합 규칙 관리**: 학습된 토크나이저를 저장하고 불러와 재사용할 수 있습니다. 이를 통해 동일한 모델이나 말뭉치에 대해 일관된 토큰화를 유지할 수 있습니다.\n","\n","6. **다양한 언어 지원**:\n","   - Hugging Face의 `tokenizers`는 영어뿐만 아니라 다국어 지원을 위해 설계되었습니다. 특히 BPE와 같은 서브워드 기반 토크나이징 방법은 언어에 구애받지 않고 매우 효율적으로 동작합니다.\n","\n","7. **인터페이스의 유연성**:\n","   - Python, Rust에서 사용할 수 있으며, Python과 쉽게 연동할 수 있어 NLP 연구자와 개발자들이 손쉽게 사용할 수 있습니다.\n","   - 기본적으로 제공하는 API는 간단하면서도 강력하여, 토크나이저의 동작 방식을 상세하게 설정할 수 있습니다.\n","\n","### 주요 클래스 및 함수\n","\n","1. **ByteLevelBPETokenizer**:\n","   - BPE(Byte-Pair Encoding)를 사용하는 토크나이저로, 바이트 레벨에서 텍스트를 처리할 수 있습니다. 유니코드 문자를 포함한 모든 텍스트를 처리하는 데 유용합니다.\n","   \n","2. **BertWordPieceTokenizer**:\n","   - BERT 모델에서 사용되는 WordPiece 방식의 토크나이저로, BERT 학습에 최적화된 방식입니다.\n","   \n","3. **SentencePieceBPETokenizer**:\n","   - Google의 SentencePiece 알고리즘을 사용한 BPE 기반의 토크나이저로, 일반적으로 사전 학습 없이 바로 토크나이저를 만들 수 있어 다양한 언어에서 유연하게 사용할 수 있습니다.\n","\n","4. **train()**:\n","   - 주어진 텍스트 데이터로부터 토크나이저를 학습시키는 함수입니다. 어휘 집합 크기, 특수 토큰 등을 설정하여 학습할 수 있습니다.\n","\n","5. **encode() / decode()**:\n","   - **`encode()`**: 텍스트 데이터를 토큰으로 변환합니다.\n","   - **`decode()`**: 토큰 데이터를 다시 원래 텍스트로 복원합니다.\n","\n","### 사용 예시\n","\n","```python\n","from tokenizers import ByteLevelBPETokenizer\n","\n","# Byte-level BPE 토크나이저 생성\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# 말뭉치를 이용해 토크나이저 학습\n","tokenizer.train(files=[\"data.txt\"], vocab_size=8000, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n","\n","# 텍스트 토큰화\n","tokens = tokenizer.encode(\"Hugging Face's tokenizers library is very fast!\")\n","print(tokens.ids)  # 토큰 ID 출력\n","print(tokens.tokens)  # 토큰 출력\n","```\n","\n","이 코드는 `ByteLevelBPETokenizer`로 텍스트 데이터를 학습하고, 해당 텍스트를 토큰으로 변환하는 과정입니다.\n","\n","### 결론\n","Hugging Face의 **`tokenizers`** 라이브러리는 속도, 메모리 효율성, 유연성 면에서 매우 뛰어난 NLP 전처리 도구입니다. 다양한 토크나이저 기법을 지원하며, 이를 통해 NLP 모델을 위한 효율적인 데이터 전처리를 수행할 수 있습니다. 특히 대용량 데이터와 함께 사용하는 상황에서 매우 유용하게 사용됩니다."],"metadata":{"id":"QVPYvqzTJW13"}},{"cell_type":"markdown","source":["# 간단한 데이터 생성\n","sample_text = \"\"\"\n","Hugging Face provides state-of-the-art machine learning models.\n","The tokenizers library is very fast and efficient.\n","Natural Language Processing (NLP) has become more accessible with Hugging Face's tools.\n","\"\"\""],"metadata":{"id":"U2BTKAlmJaGC"}},{"cell_type":"code","source":["from tokenizers import ByteLevelBPETokenizer\n","\n","# 샘플 데이터\n","sample_text = \"\"\"\n","Hugging Face provides state-of-the-art machine learning models.\n","The tokenizers library is very fast and efficient.\n","Natural Language Processing (NLP) has become more accessible with Hugging Face's tools.\n","\"\"\"\n","\n","# 데이터를 파일로 저장\n","with open(\"data.txt\", 'w') as f:\n","    f.write(sample_text)\n","\n","# Byte-level BPE 토크나이저 생성\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# 말뭉치를 이용해 토크나이저 학습\n","tokenizer.train(files=[\"data.txt\"], vocab_size=8000, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n","\n","# 텍스트 토큰\n","tokens = tokenizer.encode(\"Hugging Face's tokenizers library is very fast!\")\n","print(tokens.ids)  # 토큰 ID 출력\n","print(tokens.tokens)  # 토큰 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPXatWz7JaiP","executionInfo":{"status":"ok","timestamp":1724985561877,"user_tz":-540,"elapsed":371,"user":{"displayName":"김홍준","userId":"01118584956690988204"}},"outputId":"e77ed848-44eb-4af8-9845-125886957cef"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[287, 286, 10, 86, 282, 78, 272, 76, 93, 273, 86, 224, 79, 276, 85, 263, 92, 224, 76, 86, 224, 89, 273, 92, 224, 73, 269, 87, 4]\n","['Hugging', 'ĠFace', \"'\", 's', 'Ġto', 'k', 'en', 'i', 'z', 'er', 's', 'Ġ', 'l', 'ib', 'r', 'ar', 'y', 'Ġ', 'i', 's', 'Ġ', 'v', 'er', 'y', 'Ġ', 'f', 'as', 't', '!']\n"]}]},{"cell_type":"markdown","source":["#### 학습 과정\n","이 코드의 train() 함수는 다음과 같은 과정을 통해 토크나이저를 학습시킵니다:\n","\n","- 말뭉치 분석: 지정된 텍스트 파일(여기서는 data.txt)을 읽어 들이고, 그 안에 포함된 텍스트 데이터를 분석합니다.\n","- 빈도 기반 서브워드 추출: 입력된 텍스트에서 자주 등장하는 단어 또는 서브워드를 계산하여 서브워드 단위로 분해합니다. BPE 같은 알고리즘을 사용하여 자주 등장하는 문자 쌍을 병합하여 어휘 집합을 구성합니다.\n","- 어휘 집합 생성: 설정된 vocab_size에 맞게 가장 빈도가 높은 서브워드들을 선택하여 최종 어휘집을 만듭니다.\n","- 특수 토큰 추가: 지정된 special_tokens를 어휘 집합에 추가하여, 문장 시작, 종료, 패딩, 알 수 없는 단어 등을 처리할 수 있도록 합니다.\n","- 모델 저장: 학습된 토크나이저와 어휘 집합을 모델로 저장하거나, 나중에 텍스트를 처리할 때 사용할 수 있게 합니다.\n","\n","이 학습 과정을 거치면, 토크나이저는 주어진 텍스트 파일에서 학습된 어휘 집합을 가지고 문장을 효과적으로 토큰화할 수 있게 됩니다. 특히, 특수 토큰이 추가되어 다양한 NLP 작업에서 필요에 맞게 텍스트를 처리하고, 모델이 학습하거나 예측할 수 있는 형태로 데이터를 전처리할 수 있습니다.\n","\n","이 코드는 주로 Transformer 모델과 같은 딥러닝 기반의 NLP 작업에서 사용되며, 텍스트를 토큰화하여 모델이 이해할 수 있는 형태로 변환하는 데 중요한 역할을 합니다."],"metadata":{"id":"hCpjlCYsLs1q"}},{"cell_type":"markdown","source":["### Hugging Face의 'transformers' 기반 번역 챗봇(ko to en)\n","- 모델은 OPUS-MT 프로젝트의 일부로 한영 번역 작업을 위해 사전 학습된 'Helsinki-NLP/opus-mt-ko-en' 사용\n","- AutoTokenizer: 이 클래스는 모델에 대한 텍스트 입력 전처리를 담당합니다. 텍스트를 토큰으로 분할하고, 모델에 필요한 특수 토큰을 추가하고, 토큰을 모델 어휘의 해당 ID로 변환하는 등 텍스트를 모델이 이해할 수 있는 형식으로 변환(토큰화)\n","- AutoModelForSeq2SeqLM: 이 클래스는 번역을 포함하는 시퀀스 간 언어 모델링에 적합한 모델 아키텍처를 로드합니다. 이 모델은 번역과 같은 작업에서 변환기가 작동하는 방식을 이해하는 데 기본이 되는 인코더-디코더 구조를 사용\n","-  KoreanToEnglishTranslator 인스턴스가 생성되면 지정된 model_name을 사용하여 토크나이저와 모델을 초기화\n","- 토큰화: 한국어 텍스트를 모델이 처리할 수 있는 형식으로 토큰화합니다. return_tensors=\"pt\"는 출력이 PyTorch 텐서임을 나타내며 padding=True는 모든 시퀀스가 ​​동일한 길이로 채워지도록 보장\n","- 모델 생성: 'generate' 메소드가 모델 객체에서 호출됩니다. 이 방법은 인코더가 먼저 입력 시퀀스(한국어 텍스트)를 일련의 표현으로 인코딩하는 전체 시퀀스 간 생성 프로세스를 캡슐화합니다. 그런 다음 디코더는 이러한 표현을 사용하여 한 번에 하나의 토큰씩 출력 시퀀스(영어 번역)를 생성\n","- 디코딩: decode 메서드는 생성된 토큰 ID를 다시 읽을 수 있는 텍스트로 변환하고 처리에 사용된 특수 토큰을 생략\n","\n","\n"],"metadata":{"id":"ANzfPQv2Zb6U"}}]}